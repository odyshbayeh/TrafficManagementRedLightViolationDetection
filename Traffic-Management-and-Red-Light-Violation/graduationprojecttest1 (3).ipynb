{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12270114,"sourceType":"datasetVersion","datasetId":6870687}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ultralytics Download","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics --quiet\nprint(\"Ultralytics installed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics --quiet\nprint(\"Ultralytics installed successfully!\")\n\n!wget -O yolov8s-visdrone.pt \\\n  https://huggingface.co/mshamrai/yolov8s-visdrone/resolve/main/best.pt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Yolov5 Download","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git\n%cd yolov5\n!pip install -r requirements.txt\n%cd ..\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# YOLOv5 ATTEMPT","metadata":{}},{"cell_type":"markdown","source":"## PHASE1","metadata":{}},{"cell_type":"code","source":"import gc\nimport cv2\nimport os\n\n###############################################\n# PHASE 1) Split the Large Video into Chunks\n###############################################\n# 1) Create/ensure the output folder \"clips/\" exists\noutput_dir = \"clips\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 2) Open your video\nvideo_path = \"/kaggle/input/videos/output_video.mp4\"\ncap = cv2.VideoCapture(video_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nchunk_size = 900  # 900 frames => ~30 sec if ~30 fps\nchunk_index = 0\nchunk_paths = []\n\nwhile True:\n    frames_read = 0\n    \n    # 3) Save each chunk inside the \"clips/\" folder\n    chunk_name = os.path.join(output_dir, f\"chunk_{chunk_index}.mp4\")\n    \n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(chunk_name, fourcc, fps, (width, height))\n    \n    while frames_read < chunk_size:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        writer.write(frame)\n        frames_read += 1\n    \n    writer.release()\n    \n    if frames_read == 0:\n        # No more frames => remove empty file if created\n        if os.path.exists(chunk_name):\n            os.remove(chunk_name)\n        break\n    \n    chunk_paths.append(chunk_name)\n    print(f\"PHASE 1: Created {chunk_name} with {frames_read} frames.\")\n    \n    del writer\n    gc.collect()\n    \n    chunk_index += 1\n\ncap.release()\ngc.collect()\n\nprint(\"\\nPHASE 1 COMPLETE.\")\nprint(\"Chunks:\", chunk_paths)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PHASE2","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport shutil\nimport subprocess\n\n# Path to your YOLOv5 weights\nyolov5_weights = \"/kaggle/input/videos/best_visdrone.pt\"\n\n# Directory with chunked videos\nclips_dir = \"/kaggle/working/clips\"\n\nif not os.path.exists(clips_dir):\n    raise FileNotFoundError(f\"Clips directory not found: {clips_dir}\")\n\n# Build a sorted list of chunk file paths (e.g., chunk_0.mp4, chunk_1.mp4, etc.)\ndef extract_index(filepath):\n    basename = os.path.basename(filepath)\n    # chunk_XX.mp4 => extract XX\n    try:\n        index_str = basename.split(\"_\")[1].split(\".\")[0]\n        return int(index_str)\n    except Exception:\n        return 999999\n\nchunk_files = [os.path.join(clips_dir, f) \n               for f in os.listdir(clips_dir) \n               if f.startswith(\"chunk_\") and f.endswith(\".mp4\")]\nchunk_files.sort(key=extract_index)\n\nprint(\"Found chunk files:\")\nfor f in chunk_files:\n    print(\"  \", f)\n\n\n###############################################\n# PHASE 2) Run YOLOv5 on Each Chunk\n###############################################\nannotated_chunks = []\n\nfor i, chunk_path in enumerate(chunk_files):\n    print(f\"\\nPHASE 2: Running YOLOv5 on {chunk_path} (chunk index={i})\")\n\n    # We'll store each chunk output in \"yolo_outputs/chunk_i\"\n    save_dir = f\"yolo_outputs/chunk_{i}\"\n\n    # Build the command to run YOLOv5 detect.py\n    detect_cmd = [\n        \"python\",\n        \"yolov5/detect.py\",\n        \"--weights\", \"yolov5n.pt\",\n        \"--source\", chunk_path,\n        \"--conf-thres\", \"0.27\",\n        \"--project\", \"yolo_outputs\",\n        \"--name\", f\"chunk_{i}\",\n        \"--exist-ok\",\n        \"--hide-labels\",   # turn off class labels\n        \"--hide-conf\"      # (optional) turn off confidence scores\n    ]\n\n    # Run the detection\n    subprocess.run(detect_cmd, check=False)\n\n    # YOLOv5 should create a folder: yolo_outputs/chunk_i\n    print(f\"[Chunk {i}] YOLOv5 output folder: {save_dir}\")\n    \n    # Find the new annotated mp4 file inside that folder\n    mp4_files = [f for f in os.listdir(save_dir) if f.lower().endswith(\".mp4\")]\n    \n    if not mp4_files:\n        print(f\"[Chunk {i}] No .mp4 output found in {save_dir} — skipping.\")\n        continue\n    \n    annotated_file = os.path.join(save_dir, mp4_files[0])\n    # We'll rename it to something standard\n    annotated_name = f\"chunk_{i}_annotated.mp4\"\n    os.rename(annotated_file, annotated_name)\n\n    annotated_chunks.append(annotated_name)\n    print(f\"[Chunk {i}] Annotated => {annotated_name}\")\n\n    # Optionally remove YOLOv5 output folder to save space\n    shutil.rmtree(save_dir, ignore_errors=True)\n\n    gc.collect()\n\nprint(\"\\nPHASE 2 COMPLETE.\")\nprint(\"Annotated Chunks:\", annotated_chunks)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PHASE 3 ","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport os\n\n###############################################\n# PHASE 3) Merge Annotated Chunks\n###############################################\n# Create a text file listing all annotated chunk files (with absolute paths)\nconcat_file = \"chunk_list.txt\"\nwith open(concat_file, \"w\") as f:\n    for ann_chunk in annotated_chunks:\n        f.write(f\"file '{os.path.abspath(ann_chunk)}'\\n\")\n\nfinal_output = \"final_annotated.mp4\"\n\ncmd = [\n    \"ffmpeg\", \"-y\",\n    \"-f\", \"concat\", \"-safe\", \"0\",\n    \"-i\", concat_file,\n    \"-c:v\", \"libx264\",    # re-encode video to H.264\n    \"-crf\", \"23\",         # quality parameter (lower = higher quality, bigger file)\n    \"-pix_fmt\", \"yuv420p\",# widely compatible pixel format\n    \"-an\",                # drop audio\n    final_output\n]\n\nprint(\"\\nPHASE 3: Merging annotated chunks with ffmpeg...\")\nsubprocess.run(cmd, check=True)\nprint(\"Done! Final merged file =>\", final_output)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FIXING","metadata":{}},{"cell_type":"code","source":"import gc\nimport cv2\nimport os\n\n###############################################\n# PHASE 1) Split the Large Video into Chunks\n###############################################\n# 1) Create/ensure the output folder \"clips/\" exists\noutput_dir = \"clips\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 2) Open your video\nvideo_path = \"/kaggle/input/videos/DRONEVIEW.mp4\"\ncap = cv2.VideoCapture(video_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nchunk_size = 900  # 900 frames => ~30 sec if ~30 fps\nchunk_index = 0\nchunk_paths = []\n\nwhile True:\n    frames_read = 0\n    \n    # 3) Save each chunk inside the \"clips/\" folder\n    chunk_name = os.path.join(output_dir, f\"chunk_{chunk_index}.mp4\")\n    \n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(chunk_name, fourcc, fps, (width, height))\n    \n    while frames_read < chunk_size:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        writer.write(frame)\n        frames_read += 1\n    \n    writer.release()\n    \n    if frames_read == 0:\n        # No more frames => remove empty file if created\n        if os.path.exists(chunk_name):\n            os.remove(chunk_name)\n        break\n    \n    chunk_paths.append(chunk_name)\n    print(f\"PHASE 1: Created {chunk_name} with {frames_read} frames.\")\n    \n    del writer\n    gc.collect()\n    \n    chunk_index += 1\n\ncap.release()\ngc.collect()\n\nprint(\"\\nPHASE 1 COMPLETE.\")\nprint(\"Chunks:\", chunk_paths)\n\nimport os\nimport gc\nimport shutil\nimport subprocess\n\n# Path to your YOLOv5 weights\nyolov5_weights = \"/kaggle/input/videos/best.pt\"\n\n# Directory with chunked videos\nclips_dir = \"/kaggle/working/clips\"\n\nif not os.path.exists(clips_dir):\n    raise FileNotFoundError(f\"Clips directory not found: {clips_dir}\")\n\n# Build a sorted list of chunk file paths (e.g., chunk_0.mp4, chunk_1.mp4, etc.)\ndef extract_index(filepath):\n    basename = os.path.basename(filepath)\n    # chunk_XX.mp4 => extract XX\n    try:\n        index_str = basename.split(\"_\")[1].split(\".\")[0]\n        return int(index_str)\n    except Exception:\n        return 999999\n\nchunk_files = [os.path.join(clips_dir, f) \n               for f in os.listdir(clips_dir) \n               if f.startswith(\"chunk_\") and f.endswith(\".mp4\")]\nchunk_files.sort(key=extract_index)\n\nprint(\"Found chunk files:\")\nfor f in chunk_files:\n    print(\"  \", f)\n\n\n###############################################\n# PHASE 2) Run YOLOv5 on Each Chunk\n###############################################\nannotated_chunks = []\n\nfor i, chunk_path in enumerate(chunk_files):\n    print(f\"\\nPHASE 2: Running YOLOv5 on {chunk_path} (chunk index={i})\")\n\n    # We'll store each chunk output in \"yolo_outputs/chunk_i\"\n    save_dir = f\"yolo_outputs/chunk_{i}\"\n\n    # Build the command to run YOLOv5 detect.py\n    detect_cmd = [\n        \"python\", \n        \"yolov5/detect.py\",         # path to detect.py in the cloned yolov5 folder\n        \"--weights\", yolov5_weights,\n        \"--source\", chunk_path,\n        \"--conf-thres\", \"0.25\",\n        \"--project\", \"yolo_outputs\",\n        \"--name\", f\"chunk_{i}\",\n        \"--exist-ok\",               # allow overwriting\n        # \"--save-vid\",               # save annotated .mp4\n        # optional: '--save-txt', '--save-conf', '--save-crop', etc. if needed\n    ]\n\n    # Run the detection\n    subprocess.run(detect_cmd, check=True)\n\n    # YOLOv5 should create a folder: yolo_outputs/chunk_i\n    print(f\"[Chunk {i}] YOLOv5 output folder: {save_dir}\")\n    \n    # Find the new annotated mp4 file inside that folder\n    mp4_files = [f for f in os.listdir(save_dir) if f.lower().endswith(\".mp4\")]\n    \n    if not mp4_files:\n        print(f\"[Chunk {i}] No .mp4 output found in {save_dir} — skipping.\")\n        continue\n    \n    annotated_file = os.path.join(save_dir, mp4_files[0])\n    # We'll rename it to something standard\n    annotated_name = f\"chunk_{i}_annotated.mp4\"\n    os.rename(annotated_file, annotated_name)\n\n    annotated_chunks.append(annotated_name)\n    print(f\"[Chunk {i}] Annotated => {annotated_name}\")\n\n    # Optionally remove YOLOv5 output folder to save space\n    shutil.rmtree(save_dir, ignore_errors=True)\n\n    gc.collect()\n\nprint(\"\\nPHASE 2 COMPLETE.\")\nprint(\"Annotated Chunks:\", annotated_chunks)\n\nimport subprocess\nimport os\n\n###############################################\n# PHASE 3) Merge Annotated Chunks\n###############################################\n# Create a text file listing all annotated chunk files (with absolute paths)\nconcat_file = \"chunk_list.txt\"\nwith open(concat_file, \"w\") as f:\n    for ann_chunk in annotated_chunks:\n        f.write(f\"file '{os.path.abspath(ann_chunk)}'\\n\")\n\nfinal_output = \"final_annotated.mp4\"\n\ncmd = [\n    \"ffmpeg\", \"-y\",\n    \"-f\", \"concat\", \"-safe\", \"0\",\n    \"-i\", concat_file,\n    \"-c:v\", \"libx264\",    # re-encode video to H.264\n    \"-crf\", \"23\",         # quality parameter (lower = higher quality, bigger file)\n    \"-pix_fmt\", \"yuv420p\",# widely compatible pixel format\n    \"-an\",                # drop audio\n    final_output\n]\n\nprint(\"\\nPHASE 3: Merging annotated chunks with ffmpeg...\")\nsubprocess.run(cmd, check=True)\nprint(\"Done! Final merged file =>\", final_output)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# YOLOv8 ATTEMPT","metadata":{}},{"cell_type":"markdown","source":"# PHASE 1 for v8","metadata":{}},{"cell_type":"code","source":"import gc\nimport cv2\nimport os\n\n# Output folder\noutput_dir = \"clips\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Open your video\nvideo_path = \"/kaggle/input/videos/VideoInputStream.mp4\"\ncap = cv2.VideoCapture(video_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Manually define frame ranges for each chunk\nchunk_ranges = [\n    (0, 2999),\n    (3000, 5999),\n    (6000, 8999),\n    (9000, 11999),\n    (12000, 14999),\n    (15000, 18049),\n    (18050, 20999),\n    (21000, 24099),\n    (24100, total_frames - 1)  # last chunk up to final frame\n]\n\nchunk_paths = []\n\nfor chunk_index, (start_frame, end_frame) in enumerate(chunk_ranges):\n    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n    \n    chunk_name = os.path.join(output_dir, f\"chunk_{chunk_index}.mp4\")\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(chunk_name, fourcc, fps, (width, height))\n\n    frames_read = 0\n    total_chunk_frames = end_frame - start_frame + 1\n\n    while frames_read < total_chunk_frames:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        writer.write(frame)\n        frames_read += 1\n\n    writer.release()\n    print(f\"PHASE 1: Created {chunk_name} from frame {start_frame} to {end_frame} with {frames_read} frames.\")\n\n    if frames_read == 0 and os.path.exists(chunk_name):\n        os.remove(chunk_name)\n        print(f\"⚠️ Removed empty chunk: {chunk_name}\")\n    else:\n        chunk_paths.append(chunk_name)\n\n    del writer\n    gc.collect()\n\ncap.release()\ngc.collect()\n\nprint(\"PHASE 1 COMPLETE.\")\nprint(\"Chunks created:\", chunk_paths)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PHASE 2 for v8","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport gc\nimport shutil\nfrom ultralytics import YOLO\n\n# Define the directory where your chunks are saved\nclips_dir = \"/kaggle/working/clips\"\n\n# Check if the directory exists\nif not os.path.exists(clips_dir):\n    raise FileNotFoundError(f\"Clips directory not found: {clips_dir}\")\n\n# Build a sorted list of chunk file paths (e.g., chunk_0.mp4, chunk_1.mp4, etc.)\nchunk_files = [os.path.join(clips_dir, f) for f in os.listdir(clips_dir) \n               if f.startswith(\"chunk_\") and f.endswith((\".mp4\", \".avi\"))]\n\n\ndef extract_index(filepath):\n    basename = os.path.basename(filepath)\n    try:\n        index_str = basename.split(\"_\")[1].split(\".\")[0]\n        return int(index_str)\n    except Exception:\n        return 999999\n\nchunk_files.sort(key=extract_index)\n\nprint(\"Found chunk files:\")\nfor f in chunk_files:\n    print(\"  \", f)\n\n# Initialize YOLO model\nmodel = YOLO(\"/kaggle/input/videos/best (1).pt\")\n\n# Process each chunk with YOLO\nannotated_chunks = []\nfor i, chunk in enumerate(chunk_files):\n    print(f\"\\nPHASE 2: Running YOLO on {chunk} (chunk index={i})\")\n    \n    # Force YOLO to write each chunk’s output to a unique folder\n    results = model.predict(\n        source=chunk,\n        show_labels=False,\n        conf=0.18,\n        save=True,\n        line_thickness=1,\n        project=\"yolo_outputs\",   # base folder for YOLO results\n        name=f\"chunk_{i}\",        # unique subfolder name for this chunk\n        exist_ok=True\n    )\n\n    # YOLO writes results to results[0].save_dir (e.g., \"yolo_outputs/chunk_0\")\n    save_dir = str(results[0].save_dir)\n    print(f\"[Chunk {i}] YOLO output folder: {save_dir}\")\n    \n    files_in_dir = os.listdir(save_dir)\n    print(\"YOLO wrote files:\", files_in_dir)\n    \n    # Find a .mp4 file in the folder\n    # Find an annotated video file (check both .avi and .mp4)\n    annotated_file = None\n    for f in files_in_dir:\n        if f.lower().endswith((\".mp4\", \".avi\")):  # Accept both formats\n            annotated_file = os.path.join(save_dir, f)\n            break\n            \n    if not annotated_file or not os.path.exists(annotated_file):\n        print(f\"[Chunk {i}] No .mp4 found, skipping rename.\")\n        del results\n        gc.collect()\n        continue\n    \n    # Rename/move the annotated file to a standard name in the working directory\n    annotated_name = f\"chunk_{i}_annotated.mp4\"\n    os.rename(annotated_file, annotated_name)\n    annotated_chunks.append(annotated_name)\n    print(f\"[Chunk {i}] Annotated => {annotated_name}\")\n    \n    # Clean up YOLO's output folder to free up disk space\n    shutil.rmtree(save_dir, ignore_errors=True)\n    \n    del results\n    gc.collect()\n\nprint(\"PHASE 2 COMPLETE.\")\nprint(\"Annotated Chunks:\", annotated_chunks)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PHASE 3 for v8","metadata":{}},{"cell_type":"code","source":"\nimport subprocess\nimport os\n\n# Create a text file listing all annotated chunk files (with absolute paths)\nconcat_file = \"chunk_list.txt\"\nwith open(concat_file, \"w\") as f:\n    for ann_chunk in annotated_chunks:\n        f.write(f\"file '{os.path.abspath(ann_chunk)}'\\n\")\n\nfinal_output = \"final_annotated.mp4\"\ncmd = [\n    \"ffmpeg\", \"-y\",\n    \"-f\", \"concat\", \"-safe\", \"0\",\n    \"-i\", concat_file,\n    \"-c:v\", \"libx264\",    # re-encode video to H.264\n    \"-crf\", \"23\",         # quality parameter (lower is better quality)\n    \"-pix_fmt\", \"yuv420p\",# widely compatible pixel format\n    \"-an\",                # drop audio\n    final_output\n]\n\nprint(\"\\nPHASE 3: Merging annotated chunks with ffmpeg...\")\nsubprocess.run(cmd, check=True)\nprint(\"Done! Final merged file =>\", final_output)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\n\n# Yellow→Green transition schedule (still includes all, we’ll skip 2 of them in code)\ntransition_yellow_to_green = {\n    'ID-1-F': [(945, 1020), (3945, 4020), (6945, 7020), (9945, 10020),\n               (13035, 13110), (16125, 16200), (19215, 19290),\n               (22125, 22200), (25125, 25200)],\n    'ID-1-L': [(945, 1020), (3945, 4020), (6945, 7020), (9945, 10020),\n               (13035, 13110), (16125, 16200), (19215, 19290),\n               (22125, 22200), (25125, 25200)],\n    'ID-2': [(-75, 0), (2925, 3000), (5925, 6000), (8925, 9000),\n             (11925, 12000), (14925, 15000), (17975, 18050),\n             (20925, 21000), (23975, 24050)],\n    'ID-3-F': [(1545, 1620), (4545, 4620), (7545, 7620), (10825, 10900),\n               (13635, 13710), (16725, 16800), (19815, 19890),\n               (22905, 22980), (25995, 26070)],\n    'ID-3-L': [(2325, 2400), (5325, 5400), (8325, 8400), (11325, 11400),\n               (14415, 14490), (17505, 17580), (20595, 20670),\n               (23685, 23760)],\n    'ID-4': [(435, 510), (3435, 3510), (6435, 6510), (9435, 9510),\n             (12465, 12540), (15555, 15630), (18535, 18610),\n             (21575, 21650), (24525, 24600)]\n}\n\n# Skip these panels entirely\nignored_ids = {\"ID-1-L\", \"ID-3-L\"}\n\n# Matching chunk ranges from splitting step\nchunk_ranges = [\n    (0, 2999),\n    (3000, 5999),\n    (6000, 8999),\n    (9000, 11999),\n    (12000, 14999),\n    (15000, 18049),\n    (18050, 20999),\n    (21000, 24099),\n    (24100, 26851)\n]\n\nclips_dir = \"clips\"\nchunk_files = sorted([\n    os.path.join(clips_dir, f)\n    for f in os.listdir(clips_dir)\n    if f.endswith(\".mp4\")\n])\n\n# Build global frame → list of light_ids, excluding ignored\nframe_to_lights = {}\nfor light_id, periods in transition_yellow_to_green.items():\n    if light_id in ignored_ids:\n        continue  # ⛔ skip ID-1-L and ID-3-L\n    for start, end in periods:\n        for frame in range(max(0, start), end):\n            frame_to_lights.setdefault(frame, []).append(light_id)\n\n# Flatten all target frames\nall_target_frames = sorted(frame_to_lights.keys())\nframe_ptr = 0\n\n# Process each chunk using correct start_frame\nfor chunk_idx, chunk_path in enumerate(chunk_files):\n    start_frame = chunk_ranges[chunk_idx][0]\n\n    print(f\"📦 Processing chunk {chunk_idx} ({chunk_path}) starting at frame {start_frame}\")\n    cap = cv2.VideoCapture(chunk_path)\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        global_frame = start_frame + frame_idx\n\n        while frame_ptr < len(all_target_frames) and all_target_frames[frame_ptr] == global_frame:\n            light_ids = frame_to_lights[global_frame]\n            for light_id in light_ids:\n                print(f\"✅ [Frame {global_frame}] Yellow→Green for {light_id}\")\n                save_path = f\"output_frames/chunk_{chunk_idx}/{light_id}/frame_{global_frame}.jpg\"\n                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n                cv2.imwrite(save_path, frame)\n            frame_ptr += 1\n\n        # here we want to call the yolo for the frame and save the frame outputed by the yolo along with the .txt file for the frame \n\n        frame_idx += 1\n\n    cap.release()\n\nprint(\"✅ Done — ID-1-L and ID-3-L were skipped completely.\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport json\nimport os\n\nclips_dir = \"clips\"\naoi_output = \"aoi_regions.json\"\nchunk_files = sorted([f for f in os.listdir(clips_dir) if f.endswith(\".mp4\")])\n\naoi_data = {}\n\ndef draw_polygon(chunk_idx, frame):\n    points = []\n    clone = frame.copy()\n\n    def click_event(event, x, y, flags, param):\n        if event == cv2.EVENT_LBUTTONDOWN:\n            points.append((x, y))\n            cv2.circle(clone, (x, y), 3, (0, 255, 0), -1)\n            if len(points) > 1:\n                cv2.line(clone, points[-2], points[-1], (0, 255, 0), 2)\n            cv2.imshow(\"Draw AOI - Press 's' to Save\", clone)\n\n    cv2.imshow(\"Draw AOI - Press 's' to Save\", clone)\n    cv2.setMouseCallback(\"Draw AOI - Press 's' to Save\", click_event)\n\n    while True:\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(\"s\") and len(points) >= 3:\n            aoi_data[f\"chunk_{chunk_idx}\"] = points\n            break\n        elif key == ord(\"q\"):\n            break\n\n    cv2.destroyAllWindows()\n\n\nfor idx, video_file in enumerate(chunk_files):\n    path = os.path.join(clips_dir, video_file)\n    cap = cv2.VideoCapture(path)\n    ret, frame = cap.read()\n    if not ret:\n        print(f\"❌ Failed to read {video_file}\")\n        continue\n    print(f\"\\n🔍 Drawing AOI for chunk {idx}\")\n    draw_polygon(idx, frame)\n    cap.release()\n\nwith open(aoi_output, \"w\") as f:\n    json.dump(aoi_data, f)\n\nprint(\"✅ AOI regions saved to\", aoi_output)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install shapely\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport json\nfrom shapely.geometry import Point, Polygon\nfrom ultralytics import YOLO\n\n# Load AOI\nwith open(\"aoi_regions.json\") as f:\n    aoi_regions = json.load(f)\n\n# Yellow→Green schedule (same as before)\ntransition_yellow_to_green = {\n    'ID-1-F': [...],\n    'ID-1-L': [...],\n    ...\n}\n\nignored_ids = {\"ID-1-L\", \"ID-3-L\"}\nchunk_ranges = [...]\nclips_dir = \"clips\"\n\nchunk_files = sorted([\n    os.path.join(clips_dir, f)\n    for f in os.listdir(clips_dir)\n    if f.endswith(\".mp4\")\n])\n\nmodel = YOLO(\"/kaggle/input/videos/best (1).pt\")\n\n# Build global frame → lights\nframe_to_lights = {}\nfor light_id, periods in transition_yellow_to_green.items():\n    if light_id in ignored_ids:\n        continue\n    for start, end in periods:\n        for frame in range(max(0, start), end):\n            frame_to_lights.setdefault(frame, []).append(light_id)\n\nall_target_frames = sorted(frame_to_lights.keys())\nframe_ptr = 0\n\nfor chunk_idx, chunk_path in enumerate(chunk_files):\n    start_frame = chunk_ranges[chunk_idx][0]\n    cap = cv2.VideoCapture(chunk_path)\n    print(f\"\\n📦 Processing chunk {chunk_idx}\")\n\n    polygon = Polygon(aoi_regions.get(f\"chunk_{chunk_idx}\", []))\n    best_per_light = {}\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        global_frame = start_frame + frame_idx\n\n        while frame_ptr < len(all_target_frames) and all_target_frames[frame_ptr] == global_frame:\n            light_ids = frame_to_lights[global_frame]\n            yolo_result = model.predict(frame, conf=0.18, save=False, verbose=False)[0]\n            annotated_frame = yolo_result.plot()\n\n            for light_id in light_ids:\n                base_dir = f\"output_frames/chunk_{chunk_idx}/{light_id}\"\n                os.makedirs(base_dir, exist_ok=True)\n\n                img_path = os.path.join(base_dir, f\"frame_{global_frame}.jpg\")\n                txt_path = img_path.replace(\".jpg\", \".txt\")\n                cv2.imwrite(img_path, annotated_frame)\n\n                class_0_count = 0\n                with open(txt_path, \"w\") as f:\n                    for box in yolo_result.boxes:\n                        cls = int(box.cls)\n                        x, y = box.xywh[0][:2]\n                        cx, cy = float(x), float(y)\n                        if cls == 0 and polygon.contains(Point(cx, cy)):\n                            class_0_count += 1\n                        xywhn = box.xywhn[0].tolist()\n                        f.write(f\"{cls} {' '.join(f'{x:.6f}' for x in xywhn)}\\n\")\n\n                best = best_per_light.get(light_id, (0, None, None))\n                if class_0_count > best[0]:\n                    best_out = os.path.join(base_dir, \"bestframe\")\n                    os.makedirs(best_out, exist_ok=True)\n                    with open(txt_path, \"r\") as src, open(os.path.join(best_out, \"best.txt\"), \"w\") as dst:\n                        dst.writelines(src.readlines())\n                    with open(img_path, \"rb\") as src, open(os.path.join(best_out, \"best.jpg\"), \"wb\") as dst:\n                        dst.write(src.read())\n                    best_per_light[light_id] = (class_0_count, txt_path, img_path)\n\n            frame_ptr += 1\n        frame_idx += 1\n\n    cap.release()\n\nprint(\"\\n✅ Processing complete with AOI-aware best frame selection.\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nfrom ultralytics import YOLO\n\n# Yellow→Green transition schedule\ntransition_yellow_to_green = {\n    'ID-1-F': [(945, 1020), (3945, 4020), (6945, 7020), (9945, 10020),\n               (13035, 13110), (16125, 16200), (19215, 19290),\n               (22125, 22200), (25125, 25200)],\n    'ID-1-L': [(945, 1020), (3945, 4020), (6945, 7020), (9945, 10020),\n               (13035, 13110), (16125, 16200), (19215, 19290),\n               (22125, 22200), (25125, 25200)],\n    'ID-2': [(-75, 0), (2925, 3000), (5925, 6000), (8925, 9000),\n             (11925, 12000), (14925, 15000), (17975, 18050),\n             (20925, 21000), (23975, 24050)],\n    'ID-3-F': [(1545, 1620), (4545, 4620), (7545, 7620), (10825, 10900),\n               (13635, 13710), (16725, 16800), (19815, 19890),\n               (22905, 22980), (25995, 26070)],\n    'ID-3-L': [(2325, 2400), (5325, 5400), (8325, 8400), (11325, 11400),\n               (14415, 14490), (17505, 17580), (20595, 20670),\n               (23685, 23760)],\n    'ID-4': [(435, 510), (3435, 3510), (6435, 6510), (9435, 9510),\n             (12465, 12540), (15555, 15630), (18535, 18610),\n             (21575, 21650), (24525, 24600)]\n}\n\nignored_ids = {\"ID-1-L\", \"ID-3-L\"}\n\nchunk_ranges = [\n    (0, 2999),\n    (3000, 5999),\n    (6000, 8999),\n    (9000, 11999),\n    (12000, 14999),\n    (15000, 18049),\n    (18050, 20999),\n    (21000, 24099),\n    (24100, 26851)\n]\n\nclips_dir = \"clips\"\nchunk_files = sorted([\n    os.path.join(clips_dir, f)\n    for f in os.listdir(clips_dir)\n    if f.endswith(\".mp4\")\n])\n\n# Load YOLO model once\nmodel = YOLO(\"/kaggle/input/videos/best (1).pt\")\n\n# Build global frame → list of light_ids (excluding ignored)\nframe_to_lights = {}\nfor light_id, periods in transition_yellow_to_green.items():\n    if light_id in ignored_ids:\n        continue\n    for start, end in periods:\n        for frame in range(max(0, start), end):\n            frame_to_lights.setdefault(frame, []).append(light_id)\n\n# Flatten target frames\nall_target_frames = sorted(frame_to_lights.keys())\nframe_ptr = 0\n\n# Process each chunk\nfor chunk_idx, chunk_path in enumerate(chunk_files):\n    start_frame = chunk_ranges[chunk_idx][0]\n    print(f\"\\n📦 Processing chunk {chunk_idx}: {chunk_path} (start={start_frame})\")\n    cap = cv2.VideoCapture(chunk_path)\n\n    best_per_light = {}  # {light_id: (max_count, txt_path, jpg_path)}\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        global_frame = start_frame + frame_idx\n\n        while frame_ptr < len(all_target_frames) and all_target_frames[frame_ptr] == global_frame:\n            light_ids = frame_to_lights[global_frame]\n\n            yolo_result = model.predict(frame, conf=0.18, save=False, verbose=False)[0]\n            annotated_frame = yolo_result.plot()\n\n            for light_id in light_ids:\n                print(f\"✅ [Frame {global_frame}] Detected for {light_id}\")\n\n                base_dir = f\"output_frames/chunk_{chunk_idx}/{light_id}\"\n                os.makedirs(base_dir, exist_ok=True)\n\n                img_path = os.path.join(base_dir, f\"frame_{global_frame}.jpg\")\n                txt_path = img_path.replace(\".jpg\", \".txt\")\n\n                cv2.imwrite(img_path, annotated_frame)\n\n                class_0_count = 0\n                with open(txt_path, \"w\") as f:\n                    for box in yolo_result.boxes:\n                        cls = int(box.cls)\n                        xywhn = box.xywhn[0].tolist()\n                        f.write(f\"{cls} {' '.join(f'{x:.6f}' for x in xywhn)}\\n\")\n                        if cls == 0:\n                            class_0_count += 1\n\n                current_best = best_per_light.get(light_id, (0, None, None))\n                if class_0_count > current_best[0]:\n                    print(f\"🏆 New best for {light_id} in chunk {chunk_idx}: {class_0_count} cars\")\n\n                    best_out_dir = os.path.join(base_dir, \"bestframe\")\n                    os.makedirs(best_out_dir, exist_ok=True)\n\n                    # Overwrite best.txt / best.jpg\n                    with open(txt_path, \"r\") as src, open(os.path.join(best_out_dir, \"best.txt\"), \"w\") as dst:\n                        dst.writelines(src.readlines())\n\n                    with open(img_path, \"rb\") as src, open(os.path.join(best_out_dir, \"best.jpg\"), \"wb\") as dst:\n                        dst.write(src.read())\n\n                    best_per_light[light_id] = (class_0_count, txt_path, img_path)\n\n            frame_ptr += 1\n\n        frame_idx += 1\n\n    cap.release()\n\nprint(\"\\n✅ All chunks processed, YOLO run, and best frames saved instantly during processing.\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final code working ","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2, csv, json, os\nfrom pathlib import Path\nimport numpy as np\n\n# ==================== Config Paths ====================\nMODEL_PT = \"/kaggle/input/videos/best (1).pt\"\nPOLY_CSV = \"/kaggle/input/videos/polygons.csv\"\nCHUNKS_DIR = \"/kaggle/working/clips\"\nOUT_DIR = Path(\"outputs_video\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nLIGHT_DIR = OUT_DIR / \"original_lights\"\nLIGHT_DIR.mkdir(exist_ok=True)\nCOUNTS_DIR = OUT_DIR / \"counts\"\nCOUNTS_DIR.mkdir(exist_ok=True)\nBEST_FRAME_DIR = Path(\"best_frames\")\nfor tid in [\"ID-1\", \"ID-2\", \"ID-3\", \"ID-4\"]:\n    Path(BEST_FRAME_DIR / tid).mkdir(parents=True, exist_ok=True)\n\n# ==================== Load Polygons ====================\npoly_by_frame: dict[int, list[tuple[str, np.ndarray]]] = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    rdr = csv.DictReader(f)\n    for row in rdr:\n        fidx = int(row[\"frame\"])\n        ident = row[\"id\"]\n        pts = np.array([[int(row[f\"x{i}\"]), int(row[f\"y{i}\"])] for i in range(1, 5)], dtype=np.int32)\n        poly_by_frame.setdefault(fidx, []).append((ident, pts))\n\nCOLOURS = [(0,255,0), (0,128,255), (255,0,0), (128,0,255)]\nTRAFFIC_LIGHT_STATE_COLOUR = {\"red\": (0,0,255), \"yellow\": (0,255,255), \"green\": (0,255,0), \"unknown\": (128,128,128)}\nPRIORITY = {\"red\": 3, \"yellow\": 2, \"green\": 1}\ntraffic_light_polygons = [(\"ID-1\", 15, 83, 40, 130), (\"ID-2\", 105, 83, 40, 130), (\"ID-3\", 180, 83, 40, 130), (\"ID-4\", 270, 83, 40, 130)]\n\nmodel = YOLO(MODEL_PT)\nbest_car_counts = {tid: -1 for tid in [\"ID-1\", \"ID-2\", \"ID-3\", \"ID-4\"]}\n\n# ==================== Process Chunks ====================\nfor chunk_file in sorted(os.listdir(CHUNKS_DIR)):\n    if not chunk_file.endswith(\".mp4\"):\n        continue\n    video_path = os.path.join(CHUNKS_DIR, chunk_file)\n    chunk_id = Path(chunk_file).stem.split('_')[-1]\n\n    cap = cv2.VideoCapture(video_path)\n    assert cap.isOpened(), f\"Cannot open {video_path}\"\n    w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    writer = cv2.VideoWriter(str(OUT_DIR / f\"annotated_chunk_{chunk_id}.mp4\"), cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n    frame_idx = 1\n    last_polys = []\n\n    while True:\n        ok, frame = cap.read()\n        if not ok: break\n\n        polys = poly_by_frame.get(frame_idx, last_polys)\n        if frame_idx in poly_by_frame:\n            last_polys = polys\n\n        tl_state = {tid: \"unknown\" for tid, *_ in traffic_light_polygons}\n        tl_score = {tid: 0.0 for tid in tl_state}\n        counts = {pid: 0 for pid, _ in polys}\n\n        # ==== ROI bounding box ====\n        all_pts = np.vstack([p for _, p in polys])\n        x, y, w_roi, h_roi = cv2.boundingRect(all_pts)\n        roi = frame[y:y+h_roi, x:x+w_roi].copy()\n        roi_result = model(roi, conf=0.20, verbose=False)\n        boxes = roi_result[0].boxes\n\n        # ==== Remap boxes to original frame and count ====\n        yolotxt_lines = []\n        for box in boxes:\n            cls_id = int(box.cls[0])\n            conf = float(box.conf[0])\n            x1, y1, x2, y2 = box.xyxy[0]\n            x1_full, y1_full = x1 + x, y1 + y\n            x2_full, y2_full = x2 + x, y2 + y\n            cx, cy = float((x1_full + x2_full) / 2), float((y1_full + y2_full) / 2)\n\n            yolotxt_lines.append(f\"{cls_id} {cx/w:.6f} {cy/h:.6f} {(x2_full-x1_full)/w:.6f} {(y2_full-y1_full)/h:.6f}\")\n\n            if cls_id == 0:  # car\n                for pid, poly in polys:\n                    if cv2.pointPolygonTest(poly, (cx, cy), False) >= 0:\n                        counts[pid] += 1\n                        break\n            elif cls_id in (1, 2, 3):  # traffic light\n                colour_str = {1: \"green\", 2: \"red\", 3: \"yellow\"}[cls_id]\n                for tid, px, py, pw, ph in traffic_light_polygons:\n                    if px <= cx <= px + pw and py <= cy <= py + ph:\n                        old = tl_state[tid]\n                        if (old == \"unknown\" or PRIORITY[colour_str] > PRIORITY[old] or\n                            (colour_str == old and conf > tl_score[tid])):\n                            tl_state[tid] = colour_str\n                            tl_score[tid] = conf\n                        break\n\n        # ==== Draw and Save ====\n        for i, (pid, poly) in enumerate(polys):\n            colour = COLOURS[i % len(COLOURS)]\n            cv2.polylines(frame, [poly], isClosed=True, color=colour, thickness=2)\n            cv2.putText(frame, f\"{pid}: {counts[pid]}\", (w - 300, 40 + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, colour, 3)\n\n        for tid, px, py, pw, ph in traffic_light_polygons:\n            state = tl_state[tid]\n            colour = TRAFFIC_LIGHT_STATE_COLOUR[state]\n            cv2.rectangle(frame, (px, py), (px + pw, py + ph), colour, 2)\n            cv2.putText(frame, f\"{tid}:{state}\", (px + 2, py - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour, 1)\n\n        writer.write(frame)\n\n        json_record = {\"cars\": counts, \"lights\": tl_state}\n        txt_path = COUNTS_DIR / f\"chunk{chunk_id}_frame_{frame_idx:06d}.txt\"\n        json_path = LIGHT_DIR / f\"chunk{chunk_id}_frame_{frame_idx:06d}.json\"\n        txt_path.write_text(json.dumps(counts))\n        json_path.write_text(json.dumps(json_record))\n\n        # Save YOLO txt file manually\n        yolopath = OUT_DIR / f\"chunk{chunk_id}_frame_{frame_idx:06d}.yolo.txt\"\n        yolopath.write_text(\"\\n\".join(yolotxt_lines))\n\n        # ==== Yellow → Best Frame Save Logic ====\n        for tid in tl_state:\n            if tl_state[tid] == \"yellow\":\n                car_count = sum(1 for line in yolotxt_lines if line.startswith(\"0\"))  # count class 0\n                if car_count > best_car_counts[tid]:\n                    best_car_counts[tid] = car_count\n                    cv2.imwrite(str(BEST_FRAME_DIR / tid / \"best_frame.jpg\"), frame)\n                    (BEST_FRAME_DIR / tid / \"best_frame.json\").write_text(json.dumps(json_record))\n                    (BEST_FRAME_DIR / tid / \"best_frame.txt\").write_text(\"\\n\".join(yolotxt_lines))\n\n        frame_idx += 1\n        if frame_idx % 100 == 1:\n            print(f\"[{chunk_file}] frame {frame_idx - 1}\")\n\n    cap.release()\n    writer.release()\n    print(f\"[{chunk_file}] done.\")\n\nprint(\"PHASE 2 COMPLETE ✅\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# new recommendation system ","metadata":{}},{"cell_type":"code","source":"import cv2, csv, json, os, gc\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\n\n# ==================== Config Paths ====================\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nPOLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\nCHUNKS_DIR     = \"/kaggle/working/clips\"\nOUT_DIR        = Path(\"outputs_video\")\nLIGHT_DIR      = OUT_DIR / \"original_lights\"\nCOUNTS_DIR     = OUT_DIR / \"counts\"\nBEST_FRAME_DIR = Path(\"best_frames\")\nRECO_DIR       = Path(\"recommendations\")\n\n# create output directories\nfor d in (OUT_DIR, LIGHT_DIR, COUNTS_DIR, RECO_DIR):\n    d.mkdir(parents=True, exist_ok=True)\nfor tid in [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]:\n    (BEST_FRAME_DIR / tid).mkdir(parents=True, exist_ok=True)\n\n# ==================== Load CSV Polygons (for cars) ====================\npoly_by_frame = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    rdr = csv.DictReader(f)\n    for row in rdr:\n        fidx = int(row[\"frame\"])\n        pid  = row[\"id\"]\n        pts  = np.array([[int(row[f\"x{i}\"]), int(row[f\"y{i}\"])]\n                         for i in range(1,5)], dtype=np.int32)\n        poly_by_frame.setdefault(fidx, []).append((pid, pts))\n\n# ==================== Traffic-Light ROIs ====================\ntraffic_light_polygons = [\n    (\"ID-1\",  15,  83, 40,130),\n    (\"ID-2\", 105,  83, 40,130),\n    (\"ID-3\", 180,  83, 40,130),\n    (\"ID-4\", 270,  83, 40,130),\n]\n\n# draw colors & state map\nCOLOURS        = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTRAFFIC_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY      = {\"red\":3,\"yellow\":2,\"green\":1}\n\n# ==================== Load YOLO ====================\nmodel = YOLO(MODEL_PT)\n\n# track best car counts when yellow fires\nbest_car_counts = {tid:-1 for tid,*_ in traffic_light_polygons}\n\n# recommendation weights\nWEIGHTS = {\"ID-2\":2, \"ID-4\":2, \"ID-1\":1, \"ID-3\":1}\n\n# ==================== Process Each Chunk ====================\nfor chunk_file in sorted(os.listdir(CHUNKS_DIR)):\n    if not chunk_file.endswith(\".mp4\"):\n        continue\n    chunk_id = Path(chunk_file).stem.split(\"_\")[-1]\n    cap      = cv2.VideoCapture(os.path.join(CHUNKS_DIR, chunk_file))\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n\n    writer = cv2.VideoWriter(\n        str(OUT_DIR/f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h)\n    )\n\n    frame_idx  = 1\n    last_polys = []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # get polygons for this frame (or reuse last)\n        if frame_idx in poly_by_frame:\n            polys = poly_by_frame[frame_idx]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        # YOLO on full frame\n        res   = model(frame, conf=0.20, verbose=False)[0]\n        boxes = res.boxes\n\n        # count cars in CSV ROIs\n        counts = {pid:0 for pid,_ in polys}\n        for box in boxes:\n            if int(box.cls[0]) != 0:\n                continue\n            x1,y1,x2,y2 = box.xyxy[0]\n            cx,cy = float((x1+x2)/2), float((y1+y2)/2)\n            for pid, poly in polys:\n                if cv2.pointPolygonTest(poly, (cx,cy), False) >= 0:\n                    counts[pid] += 1\n                    break\n\n        # detect TL states in each fixed rectangle\n        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n        tl_score = {tid:0.0       for tid in tl_state}\n        for box in boxes:\n            cls_id = int(box.cls[0])\n            if cls_id not in (1,2,3):\n                continue\n            colour_str = {1:\"green\", 2:\"red\", 3:\"yellow\"}[cls_id]\n            conf       = float(box.conf[0])\n            x1,y1,x2,y2 = box.xyxy[0]\n            cx,cy = float((x1+x2)/2), float((y1+y2)/2)\n            for tid, px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    old = tl_state[tid]\n                    if (old==\"unknown\"\n                        or PRIORITY[colour_str]>PRIORITY[old]\n                        or (colour_str==old and conf>tl_score[tid])):\n                        tl_state[tid]  = colour_str\n                        tl_score[tid]  = conf\n                    break\n\n        # draw annotations\n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame, [poly], True, col, 2)\n            cv2.putText(frame, f\"{pid}:{counts[pid]}\",\n                        (w-300,40+i*40), cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n        for tid,px,py,pw,ph in traffic_light_polygons:\n            col = TRAFFIC_COLOUR[tl_state[tid]]\n            cv2.rectangle(frame,(px,py),(px+pw,py+ph),col,2)\n            cv2.putText(frame, f\"{tid}:{tl_state[tid]}\",\n                        (px+2,py-6), cv2.FONT_HERSHEY_SIMPLEX,0.5,col,1)\n\n        writer.write(frame)\n\n        # conditional save ONLY when any light is yellow\n        if any(s==\"yellow\" for s in tl_state.values()):\n            # counts + lights JSON\n            (COUNTS_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.txt\")\\\n                .write_text(json.dumps(counts))\n            (LIGHT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.json\")\\\n                .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n\n            # best-frame update—now include chunk_id in filename\n            for tid in tl_state:\n                if tl_state[tid]!=\"yellow\":\n                    continue\n                c = counts.get(tid,0)\n                if c>best_car_counts[tid]:\n                    best_car_counts[tid] = c\n                    outd = BEST_FRAME_DIR/tid\n                    cv2.imwrite(str(outd/f\"chunk{chunk_id}_best_frame.jpg\"), frame)\n                    (outd/f\"chunk{chunk_id}_best_frame.json\")\\\n                      .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n                    (outd/f\"chunk{chunk_id}_best_frame.txt\")\\\n                      .write_text(json.dumps(counts))\n\n        if frame_idx % 100 == 1:\n            print(f\"[{chunk_file}] frame {frame_idx}\")\n        frame_idx += 1\n\n    cap.release()\n    writer.release()\n    gc.collect()\n    print(f\"[{chunk_file}] done.\")\n\n    # ======== RECOMMENDATIONS (4 distinct entries) ========\n    # load each ID's best data for THIS chunk\n    best_data = {}\n    for tid in [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]:\n        p = BEST_FRAME_DIR/tid/f\"chunk{chunk_id}_best_frame.json\"\n        if p.exists():\n            best_data[tid] = json.loads(p.read_text())\n        else:\n            best_data[tid] = {\"cars\":{tid:0}, \"lights\":{tid:\"unknown\"}}\n\n    weighted   = {tid: best_data[tid][\"cars\"].get(tid,0)*WEIGHTS[tid]\n                  for tid in best_data}\n    candidates = list(weighted.keys())\n    recs       = []\n\n    for current in [\"ID-2\",\"ID-4\",\"ID-1\",\"ID-3\"]:\n        recommended = max(candidates, key=lambda t: weighted[t])\n        data = best_data[recommended]\n        dur  = data[\"cars\"].get(recommended, 0) * 1.5\n\n        recs.append({\n            \"current\":      current,\n            \"recommended\":  recommended,\n            \"duration_sec\": dur,\n            # full counts + states from the recommended’s best-frame\n            \"all_counts\":   data[\"cars\"],\n            \"all_states\":   data[\"lights\"]\n        })\n\n        candidates.remove(recommended)\n\n    outp = RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\"\n    outp.write_text(json.dumps(recs, indent=2))\n    print(f\"[RECO] wrote {outp.stem}\")\n\nprint(\"ALL CHUNKS DONE ✅\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **small modified**","metadata":{}},{"cell_type":"code","source":"import cv2, csv, json, os, gc\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\n\n# ==================== Config Paths ====================\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nPOLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\nCHUNKS_DIR     = \"/kaggle/working/clips\"\nOUT_DIR        = Path(\"outputs_video\")\nLIGHT_DIR      = OUT_DIR / \"original_lights\"\nCOUNTS_DIR     = OUT_DIR / \"counts\"\nBEST_FRAME_DIR = Path(\"best_frames\")\nRECO_DIR       = Path(\"recommendations\")\n\n# create output directories\nfor d in (OUT_DIR, LIGHT_DIR, COUNTS_DIR, RECO_DIR):\n    d.mkdir(parents=True, exist_ok=True)\nfor tid in [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]:\n    (BEST_FRAME_DIR / tid).mkdir(parents=True, exist_ok=True)\n\n# ==================== Load CSV Polygons (for cars) ====================\npoly_by_frame = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    rdr = csv.DictReader(f)\n    for row in rdr:\n        fidx = int(row[\"frame\"])\n        pid  = row[\"id\"]\n        pts  = np.array([[int(row[f\"x{i}\"]), int(row[f\"y{i}\"])]\n                         for i in range(1,5)], dtype=np.int32)\n        poly_by_frame.setdefault(fidx, []).append((pid, pts))\n\n# ==================== Traffic-Light ROIs ====================\ntraffic_light_polygons = [\n    (\"ID-1\",  15,  83, 40,130),\n    (\"ID-2\", 105,  83, 40,130),\n    (\"ID-3\", 180,  83, 40,130),\n    (\"ID-4\", 270,  83, 40,130),\n]\n\n# draw colors & state map\nCOLOURS        = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTRAFFIC_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY      = {\"red\":3,\"yellow\":2,\"green\":1}\n\n# ==================== Load YOLO ====================\nmodel = YOLO(MODEL_PT)\n\n# track best car counts when yellow fires\nbest_car_counts = {tid:-1 for tid,*_ in traffic_light_polygons}\n\n# recommendation weights\nWEIGHTS = {\"ID-2\":2, \"ID-4\":2, \"ID-1\":1, \"ID-3\":1}\n\n# ==================== Process Each Chunk ====================\nfor chunk_file in sorted(os.listdir(CHUNKS_DIR)):\n    if not chunk_file.endswith(\".mp4\"):\n        continue\n    chunk_id = Path(chunk_file).stem.split(\"_\")[-1]\n    cap      = cv2.VideoCapture(os.path.join(CHUNKS_DIR, chunk_file))\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n\n    writer = cv2.VideoWriter(\n        str(OUT_DIR/f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h)\n    )\n\n    frame_idx  = 1\n    last_polys = []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # get polygons for this frame (or reuse last)\n        if frame_idx in poly_by_frame:\n            polys = poly_by_frame[frame_idx]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        # 1) YOLO on full frame\n        res   = model(frame, conf=0.20, verbose=False)[0]\n        boxes = res.boxes\n\n        # 2) DRAW ALL YOLO BOUNDING BOXES (on both cars and lights)\n        frame = res.plot(img=frame, labels=True, line_width=2)\n\n        # 3) Count cars in CSV ROIs\n        counts = {pid:0 for pid,_ in polys}\n        for box in boxes:\n            if int(box.cls[0]) != 0:\n                continue\n            x1,y1,x2,y2 = box.xyxy[0]\n            cx,cy = float((x1+x2)/2), float((y1+y2)/2)\n            for pid, poly in polys:\n                if cv2.pointPolygonTest(poly, (cx,cy), False) >= 0:\n                    counts[pid] += 1\n                    break\n\n        # 4) Detect TL states in each fixed rectangle\n        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n        tl_score = {tid:0.0       for tid in tl_state}\n        for box in boxes:\n            cls_id = int(box.cls[0])\n            if cls_id not in (1,2,3):\n                continue\n            colour_str = {1:\"green\",2:\"red\",3:\"yellow\"}[cls_id]\n            conf       = float(box.conf[0])\n            x1,y1,x2,y2 = box.xyxy[0]\n            cx,cy = float((x1+x2)/2), float((y1+y2)/2)\n            for tid, px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    old = tl_state[tid]\n                    if (old==\"unknown\"\n                        or PRIORITY[colour_str]>PRIORITY[old]\n                        or (colour_str==old and conf>tl_score[tid])):\n                        tl_state[tid]  = colour_str\n                        tl_score[tid]  = conf\n                    break\n\n        # 5) Draw ROI‐polygons and counts on top\n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame, [poly], True, col, 2)\n            cv2.putText(frame, f\"{pid}:{counts[pid]}\",\n                        (w-300,40+i*40), cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n\n        # 6) Draw traffic-light panel rectangles & state labels\n        for tid,px,py,pw,ph in traffic_light_polygons:\n            col = TRAFFIC_COLOUR[tl_state[tid]]\n            cv2.rectangle(frame,(px,py),(px+pw,py+ph),col,2)\n            cv2.putText(frame, f\"{tid}:{tl_state[tid]}\",\n                        (px+2,py-6), cv2.FONT_HERSHEY_SIMPLEX,0.5,col,1)\n\n        writer.write(frame)\n\n        # 7) Conditional save ONLY when any light is yellow\n        if any(s==\"yellow\" for s in tl_state.values()):\n            (COUNTS_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.txt\")\\\n                .write_text(json.dumps(counts))\n            (LIGHT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.json\")\\\n                .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n\n            for tid in tl_state:\n                if tl_state[tid]!=\"yellow\": \n                    continue\n                c = counts.get(tid,0)\n                if c>best_car_counts[tid]:\n                    best_car_counts[tid] = c\n                    outd = BEST_FRAME_DIR/tid\n                    cv2.imwrite(str(outd/f\"chunk{chunk_id}_best_frame.jpg\"), frame)\n                    (outd/f\"chunk{chunk_id}_best_frame.json\")\\\n                      .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n                    (outd/f\"chunk{chunk_id}_best_frame.txt\")\\\n                      .write_text(json.dumps(counts))\n\n        if frame_idx % 100 == 1:\n            print(f\"[{chunk_file}] frame {frame_idx}\")\n        frame_idx += 1\n\n    cap.release()\n    writer.release()\n    gc.collect()\n    print(f\"[{chunk_file}] done.\")\n\n    # ======== RECOMMENDATIONS (4 distinct entries) ========\n    best_data = {}\n    for tid in [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]:\n        p = BEST_FRAME_DIR/tid/f\"chunk{chunk_id}_best_frame.json\"\n        if p.exists():\n            best_data[tid] = json.loads(p.read_text())\n        else:\n            best_data[tid] = {\"cars\":{tid:0}, \"lights\":{tid:\"unknown\"}}\n\n    weighted   = {tid: best_data[tid][\"cars\"].get(tid,0)*WEIGHTS[tid]\n                  for tid in best_data}\n    candidates = list(weighted.keys())\n    recs       = []\n\n    for current in [\"ID-2\",\"ID-4\",\"ID-1\",\"ID-3\"]:\n        recommended = max(candidates, key=lambda t: weighted[t])\n        data        = best_data[recommended]\n        dur         = data[\"cars\"].get(recommended, 0) * 1.5\n\n        recs.append({\n            \"current\":      current,\n            \"recommended\":  recommended,\n            \"duration_sec\": dur,\n            \"all_counts\":   data[\"cars\"],\n            \"all_states\":   data[\"lights\"]\n        })\n        candidates.remove(recommended)\n\n    outp = RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\"\n    outp.write_text(json.dumps(recs, indent=2))\n    print(f\"[RECO] wrote {outp.stem}\")\n\nprint(\"ALL CHUNKS DONE ✅\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Wrapping it all together ","metadata":{}},{"cell_type":"code","source":"import os\nnum_cores = os.cpu_count() or 2\nprint(f\"Detected {num_cores} CPU cores\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Main Block To RUN***","metadata":{}},{"cell_type":"markdown","source":"Test1","metadata":{}},{"cell_type":"code","source":"import multiprocessing\nimport threading\nfrom pathlib import Path\n\n# your existing helpers\nfrom phase1 import split_into_chunks\nfrom management import run_traffic_management  # accepts (chunk_path, model)\nfrom violations import run_violation_detection  # accepts (chunk_path, model)\nfrom simulation import run_simulation_loop      # accepts (reco_dir, viol_dir)\nfrom webapp import app as flask_app            # your Flask app\n\n# where your files live\nVIDEO_IN      = \"/kaggle/input/videos/VideoInputStream.mp4\"\nCLIPS_DIR     = \"clips\"\nRECO_DIR      = \"recommendations\"\nVIOL_DIR      = \"violations\"\nMODEL_PATH    = \"/kaggle/input/videos/best (1).pt\"\n\ndef gpu_worker(task_queue: multiprocessing.Queue):\n    # Load YOLO model once in this process\n    from ultralytics import YOLO\n    model = YOLO(MODEL_PATH)\n    while True:\n        task = task_queue.get()\n        if task is None:\n            break\n        kind, chunk_path = task\n        if kind == \"management\":\n            run_traffic_management(chunk_path, model)\n        elif kind == \"violation\":\n            run_violation_detection(chunk_path, model)\n    print(\"GPU worker exiting.\")\n\ndef main():\n    # 1) Phase 1: split into chunks (CPU‐only)\n    chunk_paths = split_into_chunks(VIDEO_IN, CLIPS_DIR)\n\n    # 2) Start the single GPU worker\n    task_q = multiprocessing.Queue()\n    gpu_proc = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n    gpu_proc.start()\n\n    # 3) Enqueue both tasks *for each* chunk\n    for cp in chunk_paths:\n        task_q.put((\"management\", cp))\n        task_q.put((\"violation\",  cp))\n    # signal “no more tasks”\n    task_q.put(None)\n\n    # 4) Start simulation loop in a background thread (I/O‐bound)\n    sim_thread = threading.Thread(\n        target=run_simulation_loop,\n        args=(RECO_DIR, VIOL_DIR),\n        daemon=True\n    )\n    sim_thread.start()\n\n    # 5) Start Flask web server in another thread\n    flask_thread = threading.Thread(\n        target=lambda: flask_app.run(host=\"0.0.0.0\", port=8888),\n        daemon=True\n    )\n    flask_thread.start()\n\n    # 6) Wait for GPU work to finish\n    gpu_proc.join()\n    print(\"All GPU jobs done.\")\n\n    # 7) (Optional) keep the main thread alive so sim & web keep running:\n    sim_thread.join()\n    flask_thread.join()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test2","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------\n# 1) Imports & Config\n# ----------------------------------------\nimport gc, cv2, os, csv, json, threading, multiprocessing\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\n\nVIDEO_IN       = \"/kaggle/input/videos/VideoInputStream.mp4\"\nCLIPS_DIR      = \"clips\"\nPOLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nRECO_DIR       = \"recommendations\"\nVIOL_DIR       = \"violations\"\nOUT_DIR        = Path(\"outputs_video\")\nLIGHT_DIR      = OUT_DIR / \"original_lights\"\nCOUNTS_DIR     = OUT_DIR / \"counts\"\nBEST_FRAME_DIR = Path(\"best_frames\")\n\n# ----------------------------------------\n# 2) Phase 1: Splitting Logic\n# ----------------------------------------\ndef split_into_chunks(video_path, output_dir, chunk_ranges):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    paths = []\n    for idx, (start, end) in enumerate(chunk_ranges):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n        out = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n        writer = cv2.VideoWriter(out, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n        for _ in range(end - start + 1):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            writer.write(frame)\n        writer.release()\n        if os.path.exists(out) and os.path.getsize(out) > 0:\n            paths.append(out)\n        else:\n            os.remove(out)\n    cap.release()\n    return paths\n\n# ----------------------------------------\n# 3) Shared Data for Management & Violations\n# ----------------------------------------\npoly_by_frame = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    rdr = csv.DictReader(f)\n    for row in rdr:\n        idx = int(row[\"frame\"])\n        pid = row[\"id\"]\n        pts = np.array(\n            [[int(row[f\"x{i}\"]), int(row[f\"y{i}\"])] for i in range(1,5)],\n            np.int32\n        )\n        poly_by_frame.setdefault(idx, []).append((pid, pts))\n\ntraffic_light_polygons = [\n    (\"ID-1\",  15,  83, 40, 130),\n    (\"ID-2\", 105,  83, 40, 130),\n    (\"ID-3\", 180,  83, 40, 130),\n    (\"ID-4\", 270,  83, 40, 130),\n]\n\nCOLOURS      = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTL_COLOUR    = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY     = {\"red\":3,\"yellow\":2,\"green\":1}\nWEIGHTS      = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n\n# ----------------------------------------\n# 4) Phase 2: Traffic Management Function\n# ----------------------------------------\ndef run_traffic_management(chunk_path: str, model: YOLO) -> None:\n    chunk_id = Path(chunk_path).stem.split(\"_\")[-1]\n    cap = cv2.VideoCapture(chunk_path)\n    assert cap.isOpened(), f\"Cannot open {chunk_path}\"\n\n    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    writer = cv2.VideoWriter(\n        str(OUT_DIR / f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h)\n    )\n\n    best_car_counts = { tid: -1 for tid, *_ in traffic_light_polygons }\n    frame_idx, last_polys = 1, []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_idx in poly_by_frame:\n            polys = poly_by_frame[frame_idx]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        res   = model(frame, conf=0.20, verbose=False)[0]\n        boxes = res.boxes\n        frame = res.plot(img=frame, labels=True, line_width=2)\n\n        counts = { pid:0 for pid,_ in polys }\n        for box in boxes:\n            if int(box.cls[0])!=0: continue\n            x1,y1,x2,y2 = box.xyxy[0]\n            cx,cy = float((x1+x2)/2), float((y1+y2)/2)\n            for pid, poly in polys:\n                if cv2.pointPolygonTest(poly,(cx,cy),False)>=0:\n                    counts[pid]+=1\n                    break\n\n        tl_state = { tid:\"unknown\" for tid,*_ in traffic_light_polygons }\n        tl_score = { tid:0.0       for tid in tl_state }\n        for box in boxes:\n            cls_id = int(box.cls[0])\n            if cls_id not in (1,2,3): continue\n            colour = {1:\"green\",2:\"red\",3:\"yellow\"}[cls_id]\n            conf   = float(box.conf[0])\n            x1,y1,x2,y2 = box.xyxy[0]\n            cx,cy = float((x1+x2)/2), float((y1+y2)/2)\n            for tid, px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    old = tl_state[tid]\n                    if (old==\"unknown\" or PRIORITY[colour]>PRIORITY[old]\n                        or (colour==old and conf>tl_score[tid])):\n                        tl_state[tid]=colour\n                        tl_score[tid]=conf\n                    break\n\n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame,[poly],True,col,2)\n            cv2.putText(frame,f\"{pid}:{counts[pid]}\",(w-300,40+i*40),\n                        cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n\n        for tid,px,py,pw,ph in traffic_light_polygons:\n            col = TL_COLOUR[tl_state[tid]]\n            cv2.rectangle(frame,(px,py),(px+pw,py+ph),col,2)\n            cv2.putText(frame,f\"{tid}:{tl_state[tid]}\",(px+2,py-6),\n                        cv2.FONT_HERSHEY_SIMPLEX,0.5,col,1)\n\n        writer.write(frame)\n\n        if any(s==\"yellow\" for s in tl_state.values()):\n            (COUNTS_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.txt\").write_text(json.dumps(counts))\n            (LIGHT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.json\").write_text(\n                json.dumps({\"cars\":counts,\"lights\":tl_state})\n            )\n            for tid in tl_state:\n                if tl_state[tid]!=\"yellow\": continue\n                c=counts.get(tid,0)\n                if c>best_car_counts[tid]:\n                    best_car_counts[tid]=c\n                    sd=BEST_FRAME_DIR/tid\n                    cv2.imwrite(str(sd/f\"chunk{chunk_id}_best_frame.jpg\"),frame)\n                    (sd/f\"chunk{chunk_id}_best_frame.json\").write_text(\n                        json.dumps({\"cars\":counts,\"lights\":tl_state})\n                    )\n                    (sd/f\"chunk{chunk_id}_best_frame.txt\").write_text(json.dumps(counts))\n\n        frame_idx+=1\n\n    cap.release()\n    writer.release()\n    gc.collect()\n\n    best_data={}\n    for tid,*_ in traffic_light_polygons:\n        p=BEST_FRAME_DIR/tid/f\"chunk{chunk_id}_best_frame.json\"\n        best_data[tid] = json.loads(p.read_text()) if p.exists() else {\"cars\":{tid:0},\"lights\":{tid:\"unknown\"}}\n\n    weighted   = {tid:best_data[tid][\"cars\"].get(tid,0)*WEIGHTS[tid] for tid in best_data}\n    candidates = list(weighted)\n    recs=[]\n    for current in [\"ID-2\",\"ID-4\",\"ID-1\",\"ID-3\"]:\n        rec=max(candidates, key=lambda t:weighted[t])\n        data=best_data[rec]\n        recs.append({\n            \"current\":current,\n            \"recommended\":rec,\n            \"duration_sec\":data[\"cars\"].get(rec,0)*1.5,\n            \"all_counts\":data[\"cars\"],\n            \"all_states\":data[\"lights\"]\n        })\n        candidates.remove(rec)\n\n    (RECO_DIR/f\"chunk_{chunk_id}_recommendations.json\").write_text(json.dumps(recs,indent=2))\n\n# ----------------------------------------\n# 5) Phase 2b: Violation Detection\n# ----------------------------------------\ndef run_violation_detection(chunk_path: str, model: YOLO) -> None:\n    \"\"\"\n    ROI-based line-crossing + red-light violation detection:\n      - Define stop-lines just beyond each CSV ROI polygon\n      - For each frame, run YOLO, check if car bbox center crosses the line\n      - If crossed and light state == 'red', record violation with unique ID\n      - Dump per-chunk violations to JSON under VIOL_DIR\n    \"\"\"\n    # TODO: implement line coordinates from CSV polygons + traffic_light_polygons\n    # TODO: loop frames, YOLO inference, check point-line distance sign change\n    pass\n\n# ----------------------------------------\n# 6) GPU Worker Loop\n# ----------------------------------------\ndef gpu_worker(q):\n    model = YOLO(MODEL_PT)\n    while True:\n        task = q.get()\n        if task is None:\n            break\n        kind, chunk = task\n        if kind == \"management\":\n            run_traffic_management(chunk, model)\n        else:\n            run_violation_detection(chunk, model)\n\n# ----------------------------------------\n# 7) Simulation & Web App Stubs\n# ----------------------------------------\ndef run_simulation_loop(reco_dir, viol_dir):\n    while True:\n        # read JSONs, update plots or video overlay\n        pass\n\nfrom flask import Flask, jsonify\napp = Flask(__name__)\n@app.route(\"/reco/<chunk_id>\")\ndef get_reco(chunk_id):\n    data = json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\"))\n    return jsonify(data)\n\n# ----------------------------------------\n# 8) Orchestrator\n# ----------------------------------------\nif __name__ == \"__main__\":\n    # a) split\n    # use large int instead of float\n    chunk_ranges = [\n        (0,2999),(3000,5999),(6000,8999),(9000,11999),(12000,14999),\n        (15000,18049),(18050,20999),(21000,24099),(24100, 10**9)\n    ]\n    chunks = split_into_chunks(VIDEO_IN, CLIPS_DIR, chunk_ranges)\n\n    # b) start GPU process\n    task_q = multiprocessing.Queue()\n    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n    p.start()\n\n    # c) enqueue tasks\n    for c in chunks:\n        task_q.put((\"management\", c))\n        task_q.put((\"violation\", c))\n    task_q.put(None)\n\n    # d) simulation & web threads\n    threading.Thread(target=run_simulation_loop, args=(RECO_DIR, VIOL_DIR), daemon=True).start()\n    threading.Thread(target=lambda: app.run(port=8888), daemon=True).start()\n\n    # e) wait for GPU to finish\n    p.join()\n    print(\"All GPU tasks done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Violation Fixing**","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------\n# 1) Imports & Config\n# ----------------------------------------\nimport gc, cv2, os, csv, json, threading, multiprocessing, random\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\nfrom flask import Flask, jsonify\n\n# Paths & constants\nVIDEO_IN       = \"/kaggle/input/videos/VideoInputStream.mp4\"\nCLIPS_DIR      = \"clips\"\nPOLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nRECO_DIR       = Path(\"recommendations\")\nVIOL_DIR       = Path(\"violations\")\nOUT_DIR        = Path(\"outputs_video\")\nLIGHT_DIR      = OUT_DIR/\"original_lights\"\nCOUNTS_DIR     = OUT_DIR/\"counts\"\nBEST_FRAME_DIR = Path(\"best_frames\")\n\n# Ensure output dirs exist\nfor d in (CLIPS_DIR, RECO_DIR, VIOL_DIR, OUT_DIR, LIGHT_DIR, COUNTS_DIR, BEST_FRAME_DIR):\n    os.makedirs(d, exist_ok=True)\nfor tid in [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]:\n    (BEST_FRAME_DIR/tid).mkdir(exist_ok=True)\n\n# ----------------------------------------\n# 2) Splitting Logic\n# ----------------------------------------\ndef split_into_chunks(video_path, output_dir, chunk_ranges):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    paths = []\n    for idx, (start, end) in enumerate(chunk_ranges):\n        print(f\"[SPLIT] Creating chunk {idx} frames {start}→{end}\")\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n        out_path = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n        writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n        for _ in range(end - start + 1):\n            ret, frame = cap.read()\n            if not ret: break\n            writer.write(frame)\n        writer.release()\n        if os.path.exists(out_path) and os.path.getsize(out_path)>0:\n            paths.append(out_path)\n            print(f\"[SPLIT] Saved {out_path}\")\n        else:\n            print(f\"[SPLIT] Removed empty {out_path}\")\n            os.remove(out_path)\n    cap.release()\n    return paths\n\n# ----------------------------------------\n# 3) Shared Data for Management & Violations\n# ----------------------------------------\npoly_by_frame = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    for row in csv.DictReader(f):\n        fid, pid = int(row[\"frame\"]), row[\"id\"]\n        pts = np.array([[int(row[f\"x{i}\"]),int(row[f\"y{i}\"])] for i in range(1,5)], np.int32)\n        poly_by_frame.setdefault(fid, []).append((pid,pts))\n\ntraffic_light_polygons = [\n    (\"ID-1\", 15, 83, 40,130),\n    (\"ID-2\",105, 83, 40,130),\n    (\"ID-3\",180, 83, 40,130),\n    (\"ID-4\",270, 83, 40,130),\n]\nCOLOURS   = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTL_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY  = {\"red\":3,\"yellow\":2,\"green\":1}\nWEIGHTS   = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n\n# DYN_POLY_CSV = Path(\"/kaggle/input/videos/dynamic_polygons.csv\")\n# DYN_LINE_CSV = Path(\"/kaggle/input/videos/dynamic_lines.csv\")\n# dynamic_poly_by_frame = {}\n# dynamic_line_by_frame = {}\n# with open(DYN_POLY_CSV, newline=\"\") as f:\n#     for row in csv.DictReader(f):\n#         gf, tid = int(row[\"frame\"]), row[\"id\"]\n#         pts = np.array([[float(row[f\"x{i}\"]),float(row[f\"y{i}\"])] for i in range(1,5)], np.float32)\n#         dynamic_poly_by_frame.setdefault(gf, {})[tid] = pts\n# with open(DYN_LINE_CSV, newline=\"\") as f:\n#     for row in csv.DictReader(f):\n#         gf, tid = int(row[\"frame\"]), row[\"id\"]\n#         p1 = (float(row[\"x1\"]), float(row[\"y1\"]))\n#         p2 = (float(row[\"x2\"]), float(row[\"y2\"]))\n#         dynamic_line_by_frame.setdefault(gf, {})[tid] = np.array([p1,p2], np.float32)\n\n# CHUNK_RANGES = [\n#     (0,2999),(3000,5999),(6000,8999),(9000,11999),\n#     (12000,14999),(15000,18049),(18050,20999),\n#     (21000,24099),(24100,10**9)\n# ]\n\nCHUNK_RANGES = [\n    (0,2999),(3000,5999)\n]\n\n# ----------------------------------------\n# 4) Traffic Management\n# ----------------------------------------\ndef run_traffic_management(chunk_path: str, model: YOLO) -> None:\n    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n    print(f\"[MGMT][Chunk {chunk_id}] Start\")\n    cap = cv2.VideoCapture(chunk_path)\n    w,h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    writer = cv2.VideoWriter(\n        str(OUT_DIR/f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h)\n    )\n\n    # track best counts\n    best_car_counts = {tid:-1 for tid,*_ in traffic_light_polygons}\n    # remember previous TL state for yellow→green detection\n    prev_tl_state   = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n\n    frame_idx, last_polys = 1, []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # periodic status\n        if frame_idx % 200 == 1:\n            print(f\"[MGMT][Chunk {chunk_id}] Frame {frame_idx}\")\n\n        # load or reuse polygons\n        if frame_idx in poly_by_frame:\n            polys = poly_by_frame[frame_idx]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        # YOLO inference + overlay\n        res   = model(frame, conf=0.20, verbose=False)[0]\n        boxes = res.boxes\n        frame = res.plot(img=frame, labels=True, line_width=1)\n\n        # dump YOLO txt\n        yolo_lines = []\n        for b in boxes:\n            cls = int(b.cls[0])\n            x1,y1,x2,y2 = b.xyxy[0]\n            cx,cy = (x1+x2)/2, (y1+y2)/2\n            bw,bh = (x2-x1),(y2-y1)\n            yolo_lines.append(f\"{cls} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\")\n        (OUT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.yolo.txt\")\\\n          .write_text(\"\\n\".join(yolo_lines))\n\n        # count cars\n        counts = {pid:0 for pid,_ in polys}\n        for ln in yolo_lines:\n            cls,cxn,cyn,*_ = ln.split()\n            if int(cls)!=0: continue\n            cx,cy = float(cxn)*w, float(cyn)*h\n            for pid,poly in polys:\n                if cv2.pointPolygonTest(poly,(cx,cy),False)>=0:\n                    counts[pid]+=1\n                    break\n\n        # detect TL states\n        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n        for ln in yolo_lines:\n            cls,cxn,cyn,*_ = ln.split()\n            cid = int(cls)\n            if cid not in (1,2,3): continue\n            colour = {1:\"green\",2:\"red\",3:\"yellow\"}[cid]\n            cx,cy = float(cxn)*w, float(cyn)*h\n            for tid,px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    if PRIORITY[colour] > PRIORITY.get(tl_state[tid],0):\n                        tl_state[tid] = colour\n                    break\n\n        # draw counts & lights\n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame,[poly],True,col,2)\n            cv2.putText(frame,f\"{pid}:{counts[pid]}\",(w-300,40+i*40),\n                        cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n        for tid,px,py,pw,ph in traffic_light_polygons:\n            col = TL_COLOUR[tl_state[tid]]\n            cv2.rectangle(frame,(px,py),(px+pw,py+ph),col,2)\n            cv2.putText(frame,f\"{tid}:{tl_state[tid]}\",(px+2,py-6),\n                        cv2.FONT_HERSHEY_SIMPLEX,0.5,col,1)\n\n        writer.write(frame)\n\n        # save on YELLOW→GREEN transitions\n        for tid in tl_state:\n            if prev_tl_state[tid]==\"yellow\" and tl_state[tid]==\"green\":\n                print(f\"[MGMT][Chunk {chunk_id}] {tid} YELLOW→GREEN at frame {frame_idx}, saving counts & lights\")\n                # write counts\n                (COUNTS_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.txt\")\\\n                  .write_text(json.dumps(counts))\n                # write lights+counts\n                (LIGHT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.json\")\\\n                  .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n                # update best frame if needed\n                c = counts.get(tid,0)\n                if c>best_car_counts[tid]:\n                    best_car_counts[tid]=c\n                    sd = BEST_FRAME_DIR/tid\n                    cv2.imwrite(str(sd/f\"chunk{chunk_id}_best_frame.jpg\"),frame)\n                    (sd/f\"chunk{chunk_id}_best_frame.json\")\\\n                      .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n                    (sd/f\"chunk{chunk_id}_best_frame.txt\")\\\n                      .write_text(json.dumps(counts))\n                    print(f\"[MGMT][Chunk {chunk_id}] New best for {tid}: {c} cars at frame {frame_idx}\")\n\n        # remember for next frame\n        prev_tl_state = tl_state.copy()\n        frame_idx += 1\n\n    cap.release()\n    writer.release()\n    gc.collect()\n\n    # recommendations\n    print(f\"[MGMT][Chunk {chunk_id}] Generating recommendation JSON\")\n    best_data = {}\n    for tid,*_ in traffic_light_polygons:\n        p = BEST_FRAME_DIR/tid/f\"chunk{chunk_id}_best_frame.json\"\n        best_data[tid] = json.loads(p.read_text()) if p.exists() else {\"cars\":{tid:0},\"lights\":{tid:\"unknown\"}}\n    weighted   = {tid:best_data[tid][\"cars\"].get(tid,0)*WEIGHTS[tid] for tid in best_data}\n    candidates = list(weighted); recs=[]\n    for current in [\"ID-2\",\"ID-4\",\"ID-1\",\"ID-3\"]:\n        rec = max(candidates, key=lambda t: weighted[t])\n        data=best_data[rec]\n        recs.append({\n            \"current\":current,\n            \"recommended\":rec,\n            \"duration_sec\":((data[\"cars\"].get(rec,0)*2)+2),\n            \"all_counts\":data[\"cars\"],\n            \"all_states\":data[\"lights\"]\n        })\n        candidates.remove(rec)\n    (RECO_DIR/f\"chunk_{chunk_id}_recommendations.json\")\\\n      .write_text(json.dumps(recs,indent=2))\n    print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n\n# ----------------------------------------\n# 5) Violation Detection (with prints)\n# ----------------------------------------\ndef run_violation_detection(chunk_path: str, _model=None) -> None:\n    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n    start, _ = CHUNK_RANGES[chunk_id]\n    print(f\"[VIOL][Chunk {chunk_id}] Start\")\n    cap = cv2.VideoCapture(chunk_path)\n    prev_above = {tid: False for tid,*_ in traffic_light_polygons}\n    violations = []\n    frame_idx = 1\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        gf = start + frame_idx - 1\n        if frame_idx % 200 == 1:\n            print(f\"[VIOL][Chunk {chunk_id}] Frame {frame_idx} (global {gf})\")\n\n        # read YOLO .txt\n        txtp = OUT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.yolo.txt\"\n        detections = []\n        if txtp.exists():\n            for ln in txtp.read_text().splitlines():\n                cls,cxn,cyn,*_ = ln.split()\n                detections.append((int(cls), float(cxn)*w, float(cyn)*h))\n        else:\n            print(f\"[VIOL][Chunk {chunk_id}] Missing YOLO txt for frame {frame_idx}\")\n\n        # light state now\n        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n        for cls,cx,cy in detections:\n            if cls not in (1,2,3): continue\n            col = {1:\"green\",2:\"red\",3:\"yellow\"}[cls]\n            for tid,px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    if PRIORITY[col]>PRIORITY.get(tl_state[tid],0):\n                        tl_state[tid]=col\n                    break\n\n        # car bottom-centers\n        car_centers = [(cx,cy) for cls,cx,cy in detections if cls==0]\n\n        # check crossing lines\n        for tid,line in dynamic_line_by_frame.get(gf, {}).items():\n            p1,p2 = line\n            # is any car above?\n            above = any(\n                ((p2[0]-p1[0])*(y-p1[1]) - (p2[1]-p1[1])*(x-p1[0]))>0\n                for x,y in car_centers\n            )\n            if above and not prev_above[tid] and tl_state.get(tid)==\"red\":\n                vid = f\"{random.randint(0,99999):05d}\"\n                print(f\"[VIOL][Chunk {chunk_id}] 🚨 Violation {vid} at {tid} frame_local={frame_idx} global={gf}\")\n                violations.append({\n                    \"violation_id\": vid,\n                    \"intersection\": tid,\n                    \"chunk\": chunk_id,\n                    \"frame_local\": frame_idx,\n                    \"frame_global\": gf,\n                    \"light_state\": \"red\"\n                })\n            prev_above[tid] = above\n\n        frame_idx += 1\n\n    cap.release()\n    VIOL_DIR.mkdir(exist_ok=True)\n    outp = VIOL_DIR/f\"chunk_{chunk_id}_violations.json\"\n    outp.write_text(json.dumps(violations, indent=2))\n    print(f\"[VIOL][Chunk {chunk_id}] {len(violations)} violations saved\")\n\n# ----------------------------------------\n# 6) GPU Worker Loop\n# ----------------------------------------\ndef gpu_worker(q):\n    model = YOLO(MODEL_PT)\n    while True:\n        task = q.get()\n        if task is None: break\n        kind, chunk = task\n        if kind==\"management\":\n            run_traffic_management(chunk, model)\n        else:\n            print(f\"[VIOL][Chunk {chunk_id}] {len(violations)} violations saved\")\n            # run_violation_detection(chunk, model)\n\n# ----------------------------------------\n# 7) Web & Simulation Stubs\n# ----------------------------------------\napp = Flask(__name__)\n@app.route(\"/reco/<chunk_id>\")\ndef get_reco(chunk_id):\n    return jsonify(json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\")))\n\ndef run_simulation_loop(reco_dir, viol_dir):\n    while True: pass\n\n# ----------------------------------------\n# 8) Orchestrator\n# ----------------------------------------\nif __name__==\"__main__\":\n    print(\"[MAIN] Splitting video into chunks…\")\n    chunks = split_into_chunks(VIDEO_IN, CLIPS_DIR, CHUNK_RANGES)\n\n    print(\"[MAIN] Spawning GPU worker…\")\n    task_q = multiprocessing.Queue()\n    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n    p.start()\n\n    print(\"[MAIN] Queueing management + violation tasks…\")\n    for c in chunks:\n        task_q.put((\"management\", c))\n        # task_q.put((\"violation\",  c))\n    task_q.put(None)\n\n    print(\"[MAIN] Launching web & sim threads…\")\n    threading.Thread(target=run_simulation_loop, args=(RECO_DIR,VIOL_DIR), daemon=True).start()\n    threading.Thread(target=lambda: app.run(port=8888, host=\"0.0.0.0\"), daemon=True).start()\n\n    print(\"[MAIN] Waiting for GPU worker to finish…\")\n    p.join()\n    print(\"[MAIN] All GPU tasks done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Violation And Detection working**","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------\n# 1) Imports & Config\n# ----------------------------------------\nimport gc, cv2, os, csv, json, threading, multiprocessing, random\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\nfrom flask import Flask, jsonify\n\n# Paths & constants\nVIDEO_IN       = \"/kaggle/input/videos/VideoInputStream.mp4\"\nCLIPS_DIR      = \"clips\"\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nRECO_DIR       = Path(\"recommendations\")\nVIOL_DIR       = Path(\"violations\")\nOUT_DIR        = Path(\"outputs_video\")\nLIGHT_DIR      = OUT_DIR/\"original_lights\"\nCOUNTS_DIR     = OUT_DIR/\"counts\"\nBEST_FRAME_DIR = Path(\"best_frames\")\n\n# Dynamic CSVs (output of your SIFT-based tool)\nDYN_POLY_CSV = Path(\"/kaggle/input/videos/dynamic_polygons.csv\")\n# DYN_LINE_CSV = Path(\"/kaggle/input/videos/dynamic_lines.csv\")\n\n# Ensure output dirs exist\nfor d in (CLIPS_DIR, RECO_DIR, VIOL_DIR, OUT_DIR, LIGHT_DIR, COUNTS_DIR, BEST_FRAME_DIR):\n    os.makedirs(d, exist_ok=True)\nfor tid in [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]:\n    (BEST_FRAME_DIR/tid).mkdir(exist_ok=True)\n\n# ----------------------------------------\n# 2) Splitting Logic\n# ----------------------------------------\ndef split_into_chunks(video_path, output_dir, chunk_ranges):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    paths = []\n    for idx, (start, end) in enumerate(chunk_ranges):\n        print(f\"[SPLIT] Creating chunk {idx} frames {start}→{end}\")\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n        out_path = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n        writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n        for _ in range(end - start + 1):\n            ret, frame = cap.read()\n            if not ret: break\n            writer.write(frame)\n        writer.release()\n        if os.path.exists(out_path) and os.path.getsize(out_path)>0:\n            paths.append(out_path)\n            print(f\"[SPLIT] Saved {out_path}\")\n        else:\n            print(f\"[SPLIT] Removed empty {out_path}\")\n            os.remove(out_path)\n    cap.release()\n    return paths\n\n# ----------------------------------------\n# 3) Load Shared Data\n# ----------------------------------------\n# Static TL panel ROIs (unchanged)\ntraffic_light_polygons = [\n    (\"ID-1\", 15, 83, 40,130),\n    (\"ID-2\",105, 83, 40,130),\n    (\"ID-3\",180, 83, 40,130),\n    (\"ID-4\",270, 83, 40,130),\n]\nCOLOURS   = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTL_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY  = {\"red\":3,\"yellow\":2,\"green\":1}\nWEIGHTS   = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n\n# Dynamic car-ROI polygons per frame\ndynamic_poly_by_frame = {}\nwith open(DYN_POLY_CSV, newline=\"\") as f:\n    for row in csv.DictReader(f):\n        gf, tid = int(row[\"frame\"]), row[\"id\"]\n        pts = np.array([\n            [float(row[\"x1\"]), float(row[\"y1\"])],\n            [float(row[\"x2\"]), float(row[\"y2\"])],\n            [float(row[\"x3\"]), float(row[\"y3\"])],\n            [float(row[\"x4\"]), float(row[\"y4\"])]\n        ], np.float32)\n        dynamic_poly_by_frame.setdefault(gf, {})[tid] = pts\n\n# # Dynamic crossing lines per frame\n# dynamic_line_by_frame = {}\n# with open(DYN_LINE_CSV, newline=\"\") as f:\n#     for row in csv.DictReader(f):\n#         gf, tid = int(row[\"frame\"]), row[\"id\"]\n#         p1 = (float(row[\"x1\"]), float(row[\"y1\"]))\n#         p2 = (float(row[\"x2\"]), float(row[\"y2\"]))\n#         dynamic_line_by_frame.setdefault(gf, {})[tid] = np.array([p1,p2], np.float32)\n\n# Chunk boundaries\n# CHUNK_RANGES = [\n#     (0,2999),(3000,5999),(6000,8999),(9000,11999),\n#     (12000,14999),(15000,18049),(18050,20999),\n#     (21000,24099),(24100,10**9)\n# ]\n\nCHUNK_RANGES = [\n    (0,2999),(3000,5999)\n]\n\n# ----------------------------------------\n# 4) Traffic Management (uses dynamic polygons)\n# ----------------------------------------\ndef run_traffic_management(chunk_path: str, model: YOLO) -> None:\n    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n    print(f\"[MGMT][Chunk {chunk_id}] Start\")\n    cap = cv2.VideoCapture(chunk_path)\n    w,h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    writer = cv2.VideoWriter(\n        str(OUT_DIR/f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h)\n    )\n\n    best_car_counts = {tid:-1 for tid,*_ in traffic_light_polygons}\n    prev_tl_state   = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n    frame_idx = 1\n    last_polys = []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n\n        if frame_idx % 200 == 1:\n            print(f\"[MGMT][Chunk {chunk_id}] Frame {frame_idx}\")\n\n        # → dynamic polygons here ←\n        if frame_idx in dynamic_poly_by_frame:\n            polys = [\n                (tid, dynamic_poly_by_frame[frame_idx][tid].astype(np.int32))\n                for tid in dynamic_poly_by_frame[frame_idx]\n            ]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        # YOLO inference & overlay\n        res   = model(frame, conf=0.20, verbose=False)[0]\n        boxes = res.boxes\n        frame = res.plot(img=frame, labels=True, line_width=1)\n\n        # dump YOLO txt\n        yolo_lines = []\n        for b in boxes:\n            cls = int(b.cls[0])\n            x1,y1,x2,y2 = b.xyxy[0]\n            cx,cy = (x1+x2)/2, (y1+y2)/2\n            bw,bh = (x2-x1),(y2-y1)\n            yolo_lines.append(f\"{cls} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\")\n        (OUT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.yolo.txt\")\\\n          .write_text(\"\\n\".join(yolo_lines))\n\n        # count cars\n        counts = {pid:0 for pid,_ in polys}\n        for ln in yolo_lines:\n            cls,cxn,cyn,*_ = ln.split()\n            if int(cls)!=0: continue\n            cx,cy = float(cxn)*w, float(cyn)*h\n            for pid,poly in polys:\n                if cv2.pointPolygonTest(poly,(cx,cy),False)>=0:\n                    counts[pid]+=1\n                    break\n\n        # detect TL states\n        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n        for ln in yolo_lines:\n            cls,cxn,cyn,*_ = ln.split()\n            cid = int(cls)\n            if cid not in (1,2,3): continue\n            colour = {1:\"green\",2:\"red\",3:\"yellow\"}[cid]\n            cx,cy = float(cxn)*w, float(cyn)*h\n            for tid,px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    if PRIORITY[colour] > PRIORITY.get(tl_state[tid],0):\n                        tl_state[tid] = colour\n                    break\n\n        # draw counts & TL panels\n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame,[poly],True,col,2)\n            cv2.putText(frame,f\"{pid}:{counts[pid]}\",(w-300,40+i*40),\n                        cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n        for tid,px,py,pw,ph in traffic_light_polygons:\n            col = TL_COLOUR[tl_state[tid]]\n            cv2.rectangle(frame,(px,py),(px+pw,py+ph),col,2)\n            cv2.putText(frame,f\"{tid}:{tl_state[tid]}\",(px+2,py-6),\n                        cv2.FONT_HERSHEY_SIMPLEX,0.5,col,1)\n\n        writer.write(frame)\n\n        # save on YELLOW→GREEN transition\n        for tid in tl_state:\n            if prev_tl_state[tid]==\"yellow\" and tl_state[tid]==\"green\":\n                print(f\"[MGMT][Chunk {chunk_id}] {tid} Y→G at {frame_idx}, saving\")\n                (COUNTS_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.txt\")\\\n                  .write_text(json.dumps(counts))\n                (LIGHT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.json\")\\\n                  .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n                c = counts.get(tid,0)\n                if c>best_car_counts[tid]:\n                    best_car_counts[tid]=c\n                    sd = BEST_FRAME_DIR/tid\n                    cv2.imwrite(str(sd/f\"chunk{chunk_id}_best_frame.jpg\"),frame)\n                    (sd/f\"chunk{chunk_id}_best_frame.json\")\\\n                      .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n                    (sd/f\"chunk{chunk_id}_best_frame.txt\")\\\n                      .write_text(json.dumps(counts))\n                    print(f\"[MGMT][Chunk {chunk_id}] New best {tid}: {c} cars @frame {frame_idx}\")\n\n        prev_tl_state = tl_state.copy()\n        frame_idx += 1\n\n    cap.release(); writer.release(); gc.collect()\n\n    # recommendations (unchanged)\n    print(f\"[MGMT][Chunk {chunk_id}] Generating recommendation JSON\")\n    best_data = {}\n    for tid,*_ in traffic_light_polygons:\n        p = BEST_FRAME_DIR/tid/f\"chunk{chunk_id}_best_frame.json\"\n        best_data[tid] = json.loads(p.read_text()) if p.exists() else {\"cars\":{tid:0},\"lights\":{tid:\"unknown\"}}\n    weighted   = {tid:best_data[tid][\"cars\"].get(tid,0)*WEIGHTS[tid] for tid in best_data}\n    candidates = list(weighted); recs=[]\n    for current in [\"ID-2\",\"ID-4\",\"ID-1\",\"ID-3\"]:\n        rec = max(candidates, key=lambda t: weighted[t])\n        data=best_data[rec]\n        recs.append({\n            \"current\":current,\n            \"recommended\":rec,\n            \"duration_sec\":((data[\"cars\"].get(rec,0)*2)+2),\n            \"all_counts\":data[\"cars\"],\n            \"all_states\":data[\"lights\"]\n        })\n        candidates.remove(rec)\n    (RECO_DIR/f\"chunk_{chunk_id}_recommendations.json\")\\\n      .write_text(json.dumps(recs,indent=2))\n    print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n\n# ----------------------------------------\n# 5) Violation Detection (uses dynamic lines)\n# ----------------------------------------\ndef run_violation_detection(chunk_path: str, _model=None) -> None:\n    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n    start, _ = CHUNK_RANGES[chunk_id]\n    print(f\"[VIOL][Chunk {chunk_id}] Start\")\n    cap = cv2.VideoCapture(chunk_path)\n    prev_above = {tid: False for tid,*_ in traffic_light_polygons}\n    violations = []\n    frame_idx = 1\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        gf = start + frame_idx - 1\n        if frame_idx % 200 == 1:\n            print(f\"[VIOL][Chunk {chunk_id}] Frame {frame_idx} global {gf}\")\n\n        # read detections\n        txtp = OUT_DIR/f\"chunk{chunk_id}_frame_{frame_idx:06d}.yolo.txt\"\n        detections = []\n        if txtp.exists():\n            for ln in txtp.read_text().splitlines():\n                cls,cxn,cyn,*_ = ln.split()\n                detections.append((int(cls), float(cxn)*w, float(cyn)*h))\n        else:\n            print(f\"[VIOL][Chunk {chunk_id}] Missing YOLO txt @frame {frame_idx}\")\n\n        # get TL state (same logic)\n        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n        for cls,cx,cy in detections:\n            if cls not in (1,2,3): continue\n            col = {1:\"green\",2:\"red\",3:\"yellow\"}[cls]\n            for tid,px,py,pw,ph in traffic_light_polygons:\n                if px<=cx<=px+pw and py<=cy<=py+ph:\n                    if PRIORITY[col]>PRIORITY.get(tl_state[tid],0):\n                        tl_state[tid]=col\n                    break\n\n        # bottom-centers of cars\n        car_centers = [(cx,cy) for cls,cx,cy in detections if cls==0]\n\n        # check crossing lines\n        for tid,line in dynamic_line_by_frame.get(gf, {}).items():\n            p1,p2 = line\n            above = any(((p2[0]-p1[0])*(y-p1[1]) - (p2[1]-p1[1])*(x-p1[0]))>0\n                         for x,y in car_centers)\n            if above and not prev_above[tid] and tl_state.get(tid)==\"red\":\n                vid = f\"{random.randint(0,99999):05d}\"\n                print(f\"[VIOL][Chunk {chunk_id}] 🚨 Violation {vid} @{tid} frame_local={frame_idx} global={gf}\")\n                violations.append({\n                    \"violation_id\": vid,\n                    \"intersection\": tid,\n                    \"chunk\": chunk_id,\n                    \"frame_local\": frame_idx,\n                    \"frame_global\": gf,\n                    \"light_state\": \"red\"\n                })\n            prev_above[tid] = above\n\n        frame_idx += 1\n\n    cap.release()\n    VIOL_DIR.mkdir(exist_ok=True)\n    outp = VIOL_DIR/f\"chunk_{chunk_id}_violations.json\"\n    outp.write_text(json.dumps(violations, indent=2))\n    print(f\"[VIOL][Chunk {chunk_id}] {len(violations)} violations saved\")\n\n# ----------------------------------------\n# 6) GPU Worker Loop\n# ----------------------------------------\ndef gpu_worker(q):\n    model = YOLO(MODEL_PT)\n    while True:\n        task = q.get()\n        if task is None: break\n        kind, chunk = task\n        if kind == \"management\":\n            run_traffic_management(chunk, model)\n        else:\n            # run_violation_detection(chunk, model)\n            print(f\"[VIOL][Chunk {chunk_id}] {len(violations)} violations saved\")\n\n# ----------------------------------------\n# 7) Web & Simulation Stubs\n# ----------------------------------------\napp = Flask(__name__)\n@app.route(\"/reco/<chunk_id>\")\ndef get_reco(chunk_id):\n    return jsonify(json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\")))\n\ndef run_simulation_loop(reco_dir, viol_dir):\n    while True: pass\n\n# ----------------------------------------\n# 8) Orchestrator\n# ----------------------------------------\nif __name__==\"__main__\":\n    print(\"[MAIN] Splitting video into chunks…\")\n    chunks = split_into_chunks(VIDEO_IN, CLIPS_DIR, CHUNK_RANGES)\n\n    print(\"[MAIN] Spawning GPU worker…\")\n    task_q = multiprocessing.Queue()\n    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n    p.start()\n\n    print(\"[MAIN] Queueing management + violation tasks…\")\n    for c in chunks:\n        task_q.put((\"management\", c))\n        # task_q.put((\"violation\",  c))\n    task_q.put(None)\n\n    print(\"[MAIN] Launching web & sim threads…\")\n    threading.Thread(target=run_simulation_loop, args=(RECO_DIR,VIOL_DIR), daemon=True).start()\n    threading.Thread(target=lambda: app.run(port=8888, host=\"0.0.0.0\"), daemon=True).start()\n\n    print(\"[MAIN] Waiting for GPU worker to finish…\")\n    p.join()\n    print(\"[MAIN] All GPU tasks done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Solution**","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics --quiet\nprint(\"Ultralytics installed successfully!\")\n\n\n# ✅ Install FilterPy (required by SORT)\n!pip install filterpy --quiet\n\nimport os, shutil, sys\n\n# ✅ Clean up old files if they exist\nif os.path.exists('/kaggle/working/sort.py'):\n    os.remove('/kaggle/working/sort.py')\nif os.path.exists('/kaggle/working/sort'):\n    shutil.rmtree('/kaggle/working/sort')\n\n# ✅ Clone SORT repo\n!git clone https://github.com/abewley/sort.git /kaggle/working/sort\n\n# ✅ Add to Python path\nsys.path.append('/kaggle/working/sort')\n\n# ✅ Disable problematic visualization code\nsort_file = '/kaggle/working/sort/sort.py'\nwith open(sort_file, 'r') as f:\n    lines = f.readlines()\nwith open(sort_file, 'w') as f:\n    for line in lines:\n        if \"matplotlib.use('TkAgg')\" in line:\n            f.write(\"# \" + line)\n        else:\n            f.write(line)\n\n!pip install pymongo --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:09:18.716564Z","iopub.execute_input":"2025-07-06T11:09:18.716880Z","iopub.status.idle":"2025-07-06T11:09:33.963448Z","shell.execute_reply.started":"2025-07-06T11:09:18.716853Z","shell.execute_reply":"2025-07-06T11:09:33.962374Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hUltralytics installed successfully!\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\nCloning into '/kaggle/working/sort'...\nremote: Enumerating objects: 208, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 208 (delta 2), reused 1 (delta 1), pack-reused 203 (from 2)\u001b[K\nReceiving objects: 100% (208/208), 1.20 MiB | 9.07 MiB/s, done.\nResolving deltas: 100% (74/74), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Best counting**","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------\n# 1) Imports & Config\n# ----------------------------------------\n\nimport gc, cv2, os, csv, json, threading, multiprocessing, random\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\nfrom flask import Flask, jsonify\nfrom sort import Sort\nfrom pymongo import MongoClient\nimport base64\n\n# Paths & constants\nVIDEO_IN       = \"/kaggle/input/videos/VideoInputStream.mp4\"\nCLIPS_DIR      = \"clips\"\nPOLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nRECO_DIR       = Path(\"recommendations\")\nVIOL_DIR       = Path(\"violations\") # could save to database from violation's code\nOUT_DIR        = Path(\"outputs_video\") # Contains all the annotated chunks with frames for each annotated chunk\nLIGHT_DIR      = Path(\"original_lights\")\nCOUNTS_DIR     = Path(\"counts\")\nBEST_FRAME_DIR = Path(\"best_frames\")\nAnnotated_Videos = Path(\"Annotated_Videos\")\n\n# mongo setup\nMONGO_URI = os.getenv(\n    \"MONGO_URI\",\n    \"mongodb+srv://nafe:0597785625nafe@coffeeshop.s8duwhp.mongodb.net/?retryWrites=true&w=majority\"\n)\n\nDB_NAME  = os.getenv(\"DB_NAME\", \"trafficmanagement\")\nCOL_NAME = os.getenv(\"COL_NAME\", \"records\")\n\nmongo_client = MongoClient(MONGO_URI)\nmongo_db     = mongo_client[DB_NAME]\nrecords      = mongo_db[COL_NAME]\n\n#Chunk boundaries\nCHUNK_RANGES = [\n    (0,2999),(3000,5999),(6000,8999),(9000,11999),\n    (12000,14999),(15000,18049),(18050,20999),\n    (21000,24099)\n]\n\n# # Chunk boundaries (global frame numbers)\n# CHUNK_RANGES = [\n#     (0,   2999),\n#     # (3000,5999),\n# ]\n\n# Ensure output dirs exist\nfor d in (CLIPS_DIR, RECO_DIR, VIOL_DIR, OUT_DIR, LIGHT_DIR, COUNTS_DIR, BEST_FRAME_DIR,Annotated_Videos):\n    os.makedirs(d, exist_ok=True)\nfor tid in (\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"):\n    (BEST_FRAME_DIR/tid).mkdir(exist_ok=True)\n\ndef ccw(A, B, C):\n    # returns True if the points A, B, C are listed in counter‐clockwise order\n    return (C[1]-A[1])*(B[0]-A[0]) > (B[1]-A[1])*(C[0]-A[0])\n\ndef segment_intersects(A, B, C, D):\n    # returns True if segment AB intersects segment CD\n    return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n\n# ----------------------------------------\n# 2) Splitting Logic\n# ----------------------------------------\ndef split_into_chunks(video_path, output_dir, chunk_ranges):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    paths = []\n    for idx, (start, end) in enumerate(chunk_ranges):\n        print(f\"[SPLIT] Creating chunk {idx} frames {start}→{end}\")\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n        out_path = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n        writer = cv2.VideoWriter(out_path,\n                                 cv2.VideoWriter_fourcc(*\"mp4v\"),\n                                 fps, (w, h))\n        for _ in range(end - start + 1):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            writer.write(frame)\n        writer.release()\n        if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n            paths.append(out_path)\n            print(f\"[SPLIT] Saved {out_path}\")\n        else:\n            print(f\"[SPLIT] Removed empty {out_path}\")\n            os.remove(out_path)\n    cap.release()\n    return paths\n\n# ----------------------------------------\n# 3) data for management (includes: polygons for both intersections & traffic lights, colours, priority and weights)\n# ----------------------------------------\npoly_by_frame = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    for row in csv.DictReader(f):\n        fid, pid = int(row[\"frame\"]), row[\"id\"]\n        pts = np.array([[float(row[f\"x{i}\"]), float(row[f\"y{i}\"])] \n                        for i in range(1,5)], np.float32)\n        poly_by_frame.setdefault(fid, []).append((pid, pts))\n\ntraffic_light_polygons = [\n    (\"ID-1\",  15,  83, 40,130),\n    (\"ID-2\", 105,  83, 40,130),\n    (\"ID-3\", 180,  83, 40,130),\n    (\"ID-4\", 270,  83, 40,130),\n]\nCOLOURS   = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTL_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY  = {\"red\":3,\"yellow\":2,\"green\":1}\nWEIGHTS   = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n\n# ----------------------------------------\n# 4) Traffic Management\n# ----------------------------------------\ndef run_traffic_management(chunk_path: str, model: YOLO) -> None:\n    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n    start, end = CHUNK_RANGES[chunk_id]\n\n    cap = cv2.VideoCapture(chunk_path)\n    w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    writer = cv2.VideoWriter(\n        str(Annotated_Videos / f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h)\n    )\n\n    # instead of Sort() use:\n    tracker = Sort(max_age=360, min_hits=0, iou_threshold=0.005)\n    crossings = {tid: 0 for tid in (\"ID-1\", \"ID-2\", \"ID-3\", \"ID-4\")}\n    for tid in crossings:\n        all_frames_dir = BEST_FRAME_DIR / tid / \"all_frames\"\n        all_frames_dir.mkdir(parents=True, exist_ok=True)\n        \n    seen_ids = {tid: set() for tid in crossings}\n    counting_active = {tid: False for tid in crossings}\n    prev_states = {tid: \"unknown\" for tid in crossings}\n    prev_tl_state = prev_states.copy()\n    best_car_counts = {tid: -1 for tid, *_ in traffic_light_polygons}\n    countdown_timer  = {tid: 0     for tid in crossings}\n\n    local_idx = 1\n    last_polys = []\n\n    last_positions = {}  # (pid, obj_id) → previous center\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        global_idx = start + local_idx - 1\n        if global_idx in poly_by_frame:\n            polys = [(pid, poly.astype(np.int32)) for pid, poly in poly_by_frame[global_idx]]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        # Run YOLO\n        res = model(frame, conf=0.2, verbose=False, show_labels=False)[0]\n        boxes = res.boxes\n        detections = []\n        for b in boxes:\n            cls = int(b.cls[0])\n            if cls != 0:\n                continue\n            x1, y1, x2, y2 = b.xyxy[0]\n            conf = b.conf[0].item()\n            detections.append([x1.item(), y1.item(), x2.item(), y2.item(), conf])\n        tracked = tracker.update(np.array(detections))\n\n        # Save YOLO format per frame\n        yolo_lines = []\n        for b in boxes:\n            cls = int(b.cls[0])\n            x1, y1, x2, y2 = b.xyxy[0]\n            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n            bw, bh = (x2 - x1), (y2 - y1)\n            yolo_lines.append(f\"{cls} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\")\n        (OUT_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.yolo.txt\").write_text(\"\\n\".join(yolo_lines))\n\n        # Count cars per polygon\n        counts = {pid: 0 for pid, _ in polys}\n        for ln in yolo_lines:\n            cls, cxn, cyn, *_ = ln.split()\n            if int(cls) != 0: continue\n            cx, cy = float(cxn) * w, float(cyn) * h\n            for pid, poly in polys:\n                if cv2.pointPolygonTest(poly, (cx, cy), False) >= 0:\n                    counts[pid] += 1\n                    break\n\n        # Detect traffic light states\n        tl_state = {tid: \"unknown\" for tid, *_ in traffic_light_polygons}\n        for ln in yolo_lines:\n            cls, cxn, cyn, *_ = ln.split()\n            cid = int(cls)\n            if cid not in (1, 2, 3): continue\n            colour = {1: \"green\", 2: \"red\", 3: \"yellow\"}[cid]\n            cx, cy = float(cxn) * w, float(cyn) * h\n            for tid, px, py, pw, ph in traffic_light_polygons:\n                if px <= cx <= px + pw and py <= cy <= py + ph:\n                    if PRIORITY[colour] > PRIORITY.get(tl_state[tid], 0):\n                        tl_state[tid] = colour\n                    break\n\n        # Manage Y→G transitions\n        for tid in tl_state:\n            if ((prev_tl_state[tid] == \"red\" and tl_state[tid] == \"yellow\"   ) or \n                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"yellow\") or \n                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"green\" ) or\n                (prev_tl_state[tid] == \"green\" and tl_state[tid] == \"green\"  )   ):\n                counting_active[tid] = True\n            elif (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"red\"):\n                countdown_timer[tid] = local_idx + 25\n            elif (tl_state[tid] == \"red\" and local_idx > countdown_timer[tid]):\n                counting_active[tid] = False\n            prev_states[tid] = tl_state[tid]\n            \n\n        # Define crossing edges\n        crossing_edges = {}\n        for pid, poly in polys:\n            if len(poly) >= 2:\n                p1, p2 = poly[0], poly[1]\n                vec = p2 - p1\n                norm = np.linalg.norm(vec)\n                if norm < 1e-5:\n                    continue\n                if pid == \"ID-1\":\n                    shrink_ratio = 0.05\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n                elif pid == \"ID-2\":\n                    shrink_ratio = 0.17\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n                elif pid == \"ID-3\":\n                    shrink_ratio = 0.2\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n                elif pid == \"ID-4\":\n                    shrink_ratio = 0.2\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n\n                \n                # Perpendicular vector for offset\n                perp = np.array([-vec[1], vec[0]]) / norm\n                if pid in (\"ID-2\", \"ID-3\", \"ID-4\"):\n                    perp = -perp\n                c1 = (p1 + perp * 5).astype(int)\n                c2 = (p2 + perp * 5).astype(int)\n                crossing_edges[pid] = (c1, c2)\n\n\n        # Crossing detection using SORT IDs\n        speed_threshold = 2.0  # px/frame\n        \n        for x1, y1, x2, y2, obj_id in tracked:\n            obj_id = int(obj_id)\n            cx = (x1 + x2) / 2\n            cy = (y1 + y2) / 2\n            center = np.array([cx, cy])\n        \n            for pid, (c1, c2) in crossing_edges.items():\n                if not counting_active[pid] or obj_id in seen_ids[pid]:\n                    continue\n        \n                key = (pid, obj_id)\n                prev_center = last_positions.get(key)\n                if prev_center is not None:\n                    # 1) speed filter\n                    speed = np.linalg.norm(center - prev_center)\n                    # if speed < speed_threshold:\n                    #     # too slow / jittery — skip\n                    #     last_positions[key] = center\n                    #     continue\n        \n                    # 2) true intersection test\n                    if segment_intersects(prev_center, center, c1, c2):\n                        crossings[pid] += 1\n                        seen_ids[pid].add(obj_id)\n        \n                # update last position for next frame\n                last_positions[key] = center\n                \n        # Overlay\n        # 1) Draw each tracked bounding box + its track ID\n        for x1, y1, x2, y2, obj_id in tracked:\n            x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 0), 2)             # cyan boxes\n            cv2.putText(frame, f\"ID{int(obj_id)}\", (x1, y1-5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 2)\n            \n        # 2) Draw each ROI polygon in a distinct color\n        for i, (pid, poly) in enumerate(last_polys):\n            col = COLOURS[i % len(COLOURS)]\n            cv2.polylines(frame, [poly], True, col, 2)\n            \n        # 3) Draw each crossing line (c1→c2)\n        for pid, (c1, c2) in crossing_edges.items():\n            cv2.line(frame, tuple(c1), tuple(c2), (255,255,255), 2)  # white line\n            # optional: label which line belongs to which PID\n            mid = ((c1+c2)//2).tolist()\n            cv2.putText(frame, pid, tuple(mid),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n            \n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame,[poly],True,col,2)\n            cv2.putText(frame,f\"{pid}:{counts[pid]}\",(w-300,40+i*40),\n                    cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n\n        writer.write(frame)\n\n        for tid in tl_state:\n            # frames_dir = BEST_FRAME_DIR / tid / \"all_frames\"\n            # frame_path = frames_dir / f\"chunk{chunk_id}_frame_{local_idx:06d}.jpg\"\n            # cv2.imwrite(str(frame_path), frame)\n        \n            # existing best-frame logic\n            if ((prev_tl_state[tid] == \"red\"    and tl_state[tid] == \"yellow\") or\n                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"yellow\")):\n                # save counts & light JSON as before\n                (COUNTS_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.txt\") \\\n                    .write_text(json.dumps(counts))\n                (LIGHT_DIR  / f\"chunk{chunk_id}_frame_{local_idx:06d}.json\") \\\n                    .write_text(json.dumps({\"cars\": counts, \"lights\": tl_state}))\n        \n                # now save best‐frame if it beats the previous record\n                c = counts.get(tid, 0)\n                if c > best_car_counts[tid]:\n                    best_car_counts[tid] = c\n                    best_dir = BEST_FRAME_DIR / tid\n                    # this remains in the parent tid folder\n                    bf_path = best_dir / f\"chunk{chunk_id}_best_frame.jpg\"\n                    cv2.imwrite(str(bf_path), frame)\n                    (best_dir / f\"chunk{chunk_id}_best_frame.json\") \\\n                      .write_text(json.dumps({\"cars\": counts, \"lights\": tl_state}))\n                    (best_dir / f\"chunk{chunk_id}_best_frame.txt\") \\\n                      .write_text(json.dumps(counts))\n\n        prev_tl_state = tl_state.copy()\n        local_idx += 1\n\n    cap.release()\n    writer.release()\n    gc.collect()\n\n    # Save chunk summary\n    (RECO_DIR / f\"crossings_chunk_{chunk_id}.json\").write_text(json.dumps(crossings, indent=2))\n\n    # === RECOMMENDATIONS ===\n    best_data = {}\n    for tid, *_ in traffic_light_polygons:\n        p = BEST_FRAME_DIR / tid / f\"chunk{chunk_id}_best_frame.json\"\n        best_data[tid] = json.loads(p.read_text()) if p.exists() else {\"cars\": {tid: 0}, \"lights\": {tid: \"unknown\"}}\n\n    weighted = {tid: best_data[tid][\"cars\"].get(tid, 0) * WEIGHTS[tid] for tid in best_data}\n    candidates = list(weighted)\n    recs = []\n\n    for current in (\"ID-4\", \"ID-1\", \"ID-3\", \"ID-2\"):\n        rec = max(candidates, key=lambda t: weighted[t])\n        data = best_data[rec]\n        all_counts = {tid: cnt + 2 for tid, cnt in data[\"cars\"].items()}\n        all_states = data[\"lights\"].copy()\n        all_states[rec] = \"yellow\"\n        recs.append({\n            \"current\": current,\n            \"recommended\": rec,\n            \"duration_sec\": (all_counts[rec] * 2 + 2),\n            \"all_counts\": all_counts,\n            \"all_states\": all_states\n        })\n        candidates.remove(rec)\n\n    (RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\").write_text(json.dumps(recs, indent=2))\n    print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n# ----------------------------------------\n# 5) (Your violation detection follows…)\n# ----------------------------------------\ndef run_violation_detection(chunk_path: str, _model=None) -> None:\n    # … unchanged …\n    pass\n\n# ----------------------------------------\n# 6) GPU Worker & Orchestrator\n# ----------------------------------------\ndef gpu_worker(q):\n    model = YOLO(MODEL_PT)\n    while True:\n        task = q.get()\n        if task is None: break\n        kind, chunk = task\n        if kind==\"management\":\n            run_traffic_management(chunk, model)\n        else:\n            print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n            # run_violation_detection(chunk, model)\n\napp = Flask(__name__)\n@app.route(\"/reco/<chunk_id>\")\ndef get_reco(chunk_id):\n    return jsonify(json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\")))\n\ndef run_simulation_loop(reco_dir, viol_dir):\n    while True: pass\n\nif __name__==\"__main__\":\n\n    # right before you call split_into_chunks(...)\n    CHUNKS_FILE = \"chunks_paths.txt\"\n    \n    if os.path.exists(CHUNKS_FILE):\n        # Load from previous run\n        with open(CHUNKS_FILE, \"r\") as f:\n            chunks = [line.strip() for line in f if line.strip()]\n        print(f\"[LOAD] Loaded {len(chunks)} chunk paths from {CHUNKS_FILE}\")\n    else:\n        # First time: actually split\n        chunks = split_into_chunks(VIDEO_IN, CLIPS_DIR, CHUNK_RANGES)\n        with open(CHUNKS_FILE, \"w\") as f:\n            for p in chunks:\n                f.write(p + \"\\n\")\n        print(f\"[SPLIT] Saved {len(chunks)} chunk paths to {CHUNKS_FILE}\")\n\n    task_q = multiprocessing.Queue()\n    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n    p.start()\n\n    for c in chunks:\n        task_q.put((\"management\", c))\n        # task_q.put((\"violation\",  c))\n    task_q.put(None)\n\n    # threading.Thread(target=run_simulation_loop,\n    #                  args=(RECO_DIR,VIOL_DIR), daemon=True).start()\n    # threading.Thread(target=lambda: app.run(port=8888, host=\"0.0.0.0\"),\n    #                  daemon=True).start()\n\n    p.join()\n    print(\"All GPU tasks done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:54:52.727361Z","iopub.execute_input":"2025-07-06T11:54:52.727747Z","iopub.status.idle":"2025-07-06T12:06:08.905942Z","shell.execute_reply.started":"2025-07-06T11:54:52.727719Z","shell.execute_reply":"2025-07-06T12:06:08.904960Z"}},"outputs":[{"name":"stdout","text":"[LOAD] Loaded 8 chunk paths from chunks_paths.txt\n[MGMT][Chunk 0] Recommendations saved\n[MGMT][Chunk 1] Recommendations saved\n","output_type":"stream"},{"name":"stderr","text":"Process Process-4:\nOSError: [Errno 28] No space left on device\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"<ipython-input-12-873e736ae3e8>\", line 406, in gpu_worker\n    run_traffic_management(chunk, model)\n  File \"<ipython-input-12-873e736ae3e8>\", line 335, in run_traffic_management\n    .write_text(json.dumps(counts))\n  File \"/usr/lib/python3.10/pathlib.py\", line 1154, in write_text\n    with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) as f:\nOSError: [Errno 28] No space left on device\n","output_type":"stream"},{"name":"stdout","text":"All GPU tasks done.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ----------------------------------------\n# READ FROM DISK & UPLOAD TO MONGODB (with real_world from crossings)\n# ----------------------------------------\nfrom pathlib import Path\nimport json, base64, os\nfrom pymongo import MongoClient\n\n# 1) Paths & constants\nCLIPS_DIR       = Path(\"clips\")\nRECO_DIR        = Path(\"recommendations\")\nBEST_FRAME_DIR  = Path(\"best_frames\")\nIDS             = [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]\n\n# 2) MongoDB setup\nMONGO_URI = os.getenv(\n    \"MONGO_URI\",\n    \"mongodb+srv://abubakernafe1:0597785625nafe@trafficmanagement.r28vab3.mongodb.net/?retryWrites=true&w=majority\"\n)\nDB_NAME  = os.getenv(\"DB_NAME\", \"trafficmanagement\")\nCOL_NAME = os.getenv(\"COL_NAME\", \"records\")\nclient   = MongoClient(MONGO_URI)\ndb       = client[DB_NAME]\nrecords  = db[COL_NAME]\n\n# 3) Find all chunks by scanning clips/\nchunk_files = sorted(CLIPS_DIR.glob(\"chunk_*.mp4\"))\nfor clip in chunk_files:\n    chunk_id = int(clip.stem.split(\"_\")[1])\n\n    # 4) Load recommendations JSON\n    rec_path = RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\"\n    if not rec_path.exists():\n        print(f\"[WARN] missing recommendations for chunk {chunk_id}, skipping\")\n        continue\n    recs = json.loads(rec_path.read_text())\n\n    # 5) Build best_frames array\n    best_frames = []\n    for tid in IDS:\n        img_file = BEST_FRAME_DIR / tid / f\"chunk{chunk_id}_best_frame.jpg\"\n        if img_file.exists():\n            b64 = base64.b64encode(img_file.read_bytes()).decode(\"utf-8\")\n        else:\n            b64 = None\n        best_frames.append({\"id\": tid, \"image\": b64})\n\n    # 6) Read real-world crossings from crossings_chunk_{n}.json\n    cross_path = RECO_DIR / f\"crossings_chunk_{chunk_id}.json\"\n    if cross_path.exists():\n        crossings = json.loads(cross_path.read_text())\n    else:\n        crossings = {tid: 0 for tid in IDS}\n\n    real_world = [\n        {\"id\": tid, \"cars_passed_in_real\": crossings.get(tid, 0)}\n        for tid in IDS\n    ]\n\n    # 7) Build upsert document\n    doc = {\n        \"chunk\":           chunk_id,\n        \"video_path\":      str(CLIPS_DIR / f\"chunk_{chunk_id}.mp4\"),\n        \"best_frames\":     best_frames,\n        \"recommendations\": recs,\n        \"real_world\":      real_world\n    }\n\n    # 8) Upsert into Mongo\n    records.update_one(\n        {\"chunk\": chunk_id},\n        {\"$set\": doc},\n        upsert=True\n    )\n    print(f\"[DB] upserted chunk {chunk_id}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ***Trying for a better value***","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------\n# 1) Imports & Config\n# ----------------------------------------\nimport gc, cv2, os, csv, json, threading, multiprocessing, random\nfrom pathlib import Path\nimport numpy as np\nfrom ultralytics import YOLO\nfrom flask import Flask, jsonify\nfrom sort import Sort\n\n# Paths & constants\nVIDEO_IN       = \"/kaggle/input/videos/VideoInputStream.mp4\"\nCLIPS_DIR      = \"clips\"\nPOLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\nMODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\nRECO_DIR       = Path(\"recommendations\")\nVIOL_DIR       = Path(\"violations\") # could save to database from violation's code\nOUT_DIR        = Path(\"outputs_video\") # Contains all the annotated chunks with frames for each annotated chunk\nLIGHT_DIR      = Path(\"original_lights\")\nCOUNTS_DIR     = Path(\"counts\")\nBEST_FRAME_DIR = Path(\"best_frames\")\nAnnotated_Videos = Path(\"Annotated_Videos\")\n\n# Chunk boundaries\n# CHUNK_RANGES = [\n#     (0,2999),(3000,5999),(6000,8999),(9000,11999),\n#     (12000,14999),(15000,18049),(18050,20999),\n#     (21000,24099),(24100,10**9)\n# ]\n\n# Chunk boundaries (global frame numbers)\nCHUNK_RANGES = [\n    (0,   2999),\n    # (3000,5999),\n]\n\n# Ensure output dirs exist\nfor d in (CLIPS_DIR, RECO_DIR, VIOL_DIR, OUT_DIR, LIGHT_DIR, COUNTS_DIR, BEST_FRAME_DIR,Annotated_Videos):\n    os.makedirs(d, exist_ok=True)\nfor tid in (\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"):\n    (BEST_FRAME_DIR/tid).mkdir(exist_ok=True)\n\ndef ccw(A, B, C):\n    # returns True if the points A, B, C are listed in counter‐clockwise order\n    return (C[1]-A[1])*(B[0]-A[0]) > (B[1]-A[1])*(C[0]-A[0])\n\ndef segment_intersects(A, B, C, D):\n    # returns True if segment AB intersects segment CD\n    return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n\n# ----------------------------------------\n# 2) Splitting Logic\n# ----------------------------------------\ndef split_into_chunks(video_path, output_dir, chunk_ranges):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    paths = []\n    for idx, (start, end) in enumerate(chunk_ranges):\n        print(f\"[SPLIT] Creating chunk {idx} frames {start}→{end}\")\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n        out_path = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n        writer = cv2.VideoWriter(out_path,\n                                 cv2.VideoWriter_fourcc(*\"mp4v\"),\n                                 fps, (w, h))\n        for _ in range(end - start + 1):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            writer.write(frame)\n        writer.release()\n        if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n            paths.append(out_path)\n            print(f\"[SPLIT] Saved {out_path}\")\n        else:\n            print(f\"[SPLIT] Removed empty {out_path}\")\n            os.remove(out_path)\n    cap.release()\n    return paths\n\n# ----------------------------------------\n# 3) data for management (includes: polygons for both intersections & traffic lights, colours, priority and weights)\n# ----------------------------------------\npoly_by_frame = {}\nwith open(POLY_CSV, newline=\"\") as f:\n    for row in csv.DictReader(f):\n        fid, pid = int(row[\"frame\"]), row[\"id\"]\n        pts = np.array([[float(row[f\"x{i}\"]), float(row[f\"y{i}\"])] \n                        for i in range(1,5)], np.float32)\n        poly_by_frame.setdefault(fid, []).append((pid, pts))\n\ntraffic_light_polygons = [\n    (\"ID-1\",  15,  83, 40,130),\n    (\"ID-2\", 105,  83, 40,130),\n    (\"ID-3\", 180,  83, 40,130),\n    (\"ID-4\", 270,  83, 40,130),\n]\nCOLOURS   = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\nTL_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\nPRIORITY  = {\"red\":3,\"yellow\":2,\"green\":1}\nWEIGHTS   = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n\n# ----------------------------------------\n# 4) Traffic Management\n# ----------------------------------------\ndef run_traffic_management(chunk_path: str, model: YOLO) -> None:\n    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n    start, end = CHUNK_RANGES[chunk_id]\n\n    cap = cv2.VideoCapture(chunk_path)\n    w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n    writer = cv2.VideoWriter(\n        str(Annotated_Videos / f\"annotated_chunk_{chunk_id}.mp4\"),\n        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h)\n    )\n\n    # instead of Sort() use:\n    tracker = Sort(max_age=360, min_hits=0, iou_threshold=0.9)\n    crossings = {tid: 0 for tid in (\"ID-1\", \"ID-2\", \"ID-3\", \"ID-4\")}\n    seen_ids = {tid: set() for tid in crossings}\n    counting_active = {tid: False for tid in crossings}\n    prev_states = {tid: \"unknown\" for tid in crossings}\n    prev_tl_state = prev_states.copy()\n    best_car_counts = {tid: -1 for tid, *_ in traffic_light_polygons}\n    countdown_timer  = {tid: 0     for tid in crossings}\n\n    local_idx = 1\n    last_polys = []\n\n    last_positions = {}  # (pid, obj_id) → previous center\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        global_idx = start + local_idx - 1\n        if global_idx in poly_by_frame:\n            polys = [(pid, poly.astype(np.int32)) for pid, poly in poly_by_frame[global_idx]]\n            last_polys = polys\n        else:\n            polys = last_polys\n\n        # Run YOLO\n        res = model(frame, conf=0.2, verbose=False, show_labels=False)[0]\n        boxes = res.boxes\n        detections = []\n        for b in boxes:\n            cls = int(b.cls[0])\n            if cls != 0:\n                continue\n            x1, y1, x2, y2 = b.xyxy[0]\n            conf = b.conf[0].item()\n            detections.append([x1.item(), y1.item(), x2.item(), y2.item(), conf])\n        tracked = tracker.update(np.array(detections))\n\n        # Save YOLO format per frame\n        yolo_lines = []\n        for b in boxes:\n            cls = int(b.cls[0])\n            x1, y1, x2, y2 = b.xyxy[0]\n            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n            bw, bh = (x2 - x1), (y2 - y1)\n            yolo_lines.append(f\"{cls} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\")\n        (OUT_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.yolo.txt\").write_text(\"\\n\".join(yolo_lines))\n\n        # Count cars per polygon\n        counts = {pid: 0 for pid, _ in polys}\n        for ln in yolo_lines:\n            cls, cxn, cyn, *_ = ln.split()\n            if int(cls) != 0: continue\n            cx, cy = float(cxn) * w, float(cyn) * h\n            for pid, poly in polys:\n                if cv2.pointPolygonTest(poly, (cx, cy), False) >= 0:\n                    counts[pid] += 1\n                    break\n\n        # Detect traffic light states\n        tl_state = {tid: \"unknown\" for tid, *_ in traffic_light_polygons}\n        for ln in yolo_lines:\n            cls, cxn, cyn, *_ = ln.split()\n            cid = int(cls)\n            if cid not in (1, 2, 3): continue\n            colour = {1: \"green\", 2: \"red\", 3: \"yellow\"}[cid]\n            cx, cy = float(cxn) * w, float(cyn) * h\n            for tid, px, py, pw, ph in traffic_light_polygons:\n                if px <= cx <= px + pw and py <= cy <= py + ph:\n                    if PRIORITY[colour] > PRIORITY.get(tl_state[tid], 0):\n                        tl_state[tid] = colour\n                    break\n\n        # Manage Y→G transitions\n        for tid in tl_state:\n            if ((prev_tl_state[tid] == \"red\" and tl_state[tid] == \"yellow\"   ) or \n                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"yellow\") or \n                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"green\" ) or\n                (prev_tl_state[tid] == \"green\" and tl_state[tid] == \"green\"  )   ):\n                counting_active[tid] = True\n            elif (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"red\"):\n                countdown_timer[tid] = local_idx + 25\n            elif (tl_state[tid] == \"red\" and local_idx > countdown_timer[tid]):\n                counting_active[tid] = False\n            prev_states[tid] = tl_state[tid]\n            \n\n        # Define crossing edges\n        crossing_edges = {}\n        for pid, poly in polys:\n            if len(poly) >= 2:\n                p1, p2 = poly[0], poly[1]\n                vec = p2 - p1\n                norm = np.linalg.norm(vec)\n                if norm < 1e-5:\n                    continue\n                if pid == \"ID-1\":\n                    shrink_ratio = 0.05\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n                elif pid == \"ID-2\":\n                    shrink_ratio = 0.17\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n                elif pid == \"ID-3\":\n                    shrink_ratio = 0.2\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n                elif pid == \"ID-4\":\n                    shrink_ratio = 0.2\n                    p1 = p1 + vec * shrink_ratio\n                    p2 = p2 - vec * shrink_ratio\n\n                \n                # Perpendicular vector for offset\n                perp = np.array([-vec[1], vec[0]]) / norm\n                if pid in (\"ID-2\", \"ID-3\", \"ID-4\"):\n                    perp = -perp\n                c1 = (p1 + perp * 3).astype(int)\n                c2 = (p2 + perp * 3).astype(int)\n                crossing_edges[pid] = (c1, c2)\n\n\n        # Crossing detection using SORT IDs\n        speed_threshold = 2.0  # px/frame\n        for x1, y1, x2, y2, obj_id in tracked:\n            obj_id = int(obj_id)\n            cx = (x1 + x2) / 2\n            cy = (y1 + y2) / 2\n            center = np.array([cx, cy])\n        \n            for pid, (c1, c2) in crossing_edges.items():\n                if not counting_active[pid] or obj_id in seen_ids[pid]:\n                    continue\n        \n                key = (pid, obj_id)\n                prev_center = last_positions.get(key)\n        \n                if prev_center is not None:\n                    # build the vector of the line\n                    v1 = c2 - c1\n                    # cross‐products at previous and current positions\n                    cross_prev = np.cross(v1, prev_center - c1)\n                    cross_now  = np.cross(v1, center      - c1)\n                    # normalized perpendicular distance of current point\n                    dist_now   = abs(cross_now) / np.linalg.norm(v1)\n                    # check either a strict crossing OR a “close enough” pass\n                    if (np.sign(cross_prev) != np.sign(cross_now)) or (dist_now < 0.5):\n                        crossings[pid] += 1\n                        seen_ids[pid].add(obj_id)\n        \n                # update for next frame\n                last_positions[key] = center\n                        \n        # Overlay\n        # 1) Draw each tracked bounding box + its track ID\n        for x1, y1, x2, y2, obj_id in tracked:\n            x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 0), 2)             # cyan boxes\n            cv2.putText(frame, f\"ID{int(obj_id)}\", (x1, y1-5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 2)\n            \n        # 2) Draw each ROI polygon in a distinct color\n        for i, (pid, poly) in enumerate(last_polys):\n            col = COLOURS[i % len(COLOURS)]\n            cv2.polylines(frame, [poly], True, col, 2)\n            \n        # 3) Draw each crossing line (c1→c2)\n        for pid, (c1, c2) in crossing_edges.items():\n            cv2.line(frame, tuple(c1), tuple(c2), (255,255,255), 2)  # white line\n            # optional: label which line belongs to which PID\n            mid = ((c1+c2)//2).tolist()\n            cv2.putText(frame, pid, tuple(mid),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n            \n        for i,(pid,poly) in enumerate(polys):\n            col = COLOURS[i%len(COLOURS)]\n            cv2.polylines(frame,[poly],True,col,2)\n            cv2.putText(frame,f\"{pid}:{counts[pid]}\",(w-300,40+i*40),\n                    cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n\n        writer.write(frame)\n\n        # Save best frame logic\n        for tid in tl_state:\n            if ((prev_tl_state[tid] == \"red\" and tl_state[tid] == \"yellow\") or (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"yellow\") ):\n                (COUNTS_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.txt\").write_text(json.dumps(counts))\n                (LIGHT_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.json\").write_text(json.dumps({\"cars\": counts, \"lights\": tl_state}))\n                c = counts.get(tid, 0)\n                if c > best_car_counts[tid]:\n                    best_car_counts[tid] = c\n                    sd = BEST_FRAME_DIR / tid\n                    cv2.imwrite(str(sd / f\"chunk{chunk_id}_best_frame.jpg\"), frame)\n                    (sd / f\"chunk{chunk_id}_best_frame.json\").write_text(json.dumps({\"cars\": counts, \"lights\": tl_state}))\n                    (sd / f\"chunk{chunk_id}_best_frame.txt\").write_text(json.dumps(counts))\n\n        prev_tl_state = tl_state.copy()\n        local_idx += 1\n\n    cap.release()\n    writer.release()\n    gc.collect()\n\n    # Save chunk summary\n    (RECO_DIR / f\"crossings_chunk_{chunk_id}.json\").write_text(json.dumps(crossings, indent=2))\n\n    # === RECOMMENDATIONS ===\n    best_data = {}\n    for tid, *_ in traffic_light_polygons:\n        p = BEST_FRAME_DIR / tid / f\"chunk{chunk_id}_best_frame.json\"\n        best_data[tid] = json.loads(p.read_text()) if p.exists() else {\"cars\": {tid: 0}, \"lights\": {tid: \"unknown\"}}\n\n    weighted = {tid: best_data[tid][\"cars\"].get(tid, 0) * WEIGHTS[tid] for tid in best_data}\n    candidates = list(weighted)\n    recs = []\n\n    for current in (\"ID-4\", \"ID-1\", \"ID-3\", \"ID-2\"):\n        rec = max(candidates, key=lambda t: weighted[t])\n        data = best_data[rec]\n        all_counts = {tid: cnt + 2 for tid, cnt in data[\"cars\"].items()}\n        all_states = data[\"lights\"].copy()\n        all_states[rec] = \"yellow\"\n        recs.append({\n            \"current\": current,\n            \"recommended\": rec,\n            \"duration_sec\": (all_counts[rec] * 2 + 2),\n            \"all_counts\": all_counts,\n            \"all_states\": all_states\n        })\n        candidates.remove(rec)\n\n    (RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\").write_text(json.dumps(recs, indent=2))\n    print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n# ----------------------------------------\n# 5) (Your violation detection follows…)\n# ----------------------------------------\ndef run_violation_detection(chunk_path: str, _model=None) -> None:\n    # … unchanged …\n    pass\n\n# ----------------------------------------\n# 6) GPU Worker & Orchestrator\n# ----------------------------------------\ndef gpu_worker(q):\n    model = YOLO(MODEL_PT)\n    while True:\n        task = q.get()\n        if task is None: break\n        kind, chunk = task\n        if kind==\"management\":\n            run_traffic_management(chunk, model)\n        else:\n            print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n            # run_violation_detection(chunk, model)\n\napp = Flask(__name__)\n@app.route(\"/reco/<chunk_id>\")\ndef get_reco(chunk_id):\n    return jsonify(json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\")))\n\ndef run_simulation_loop(reco_dir, viol_dir):\n    while True: pass\n\nif __name__==\"__main__\":\n\n    # right before you call split_into_chunks(...)\n    CHUNKS_FILE = \"chunks_paths.txt\"\n    \n    if os.path.exists(CHUNKS_FILE):\n        # Load from previous run\n        with open(CHUNKS_FILE, \"r\") as f:\n            chunks = [line.strip() for line in f if line.strip()]\n        print(f\"[LOAD] Loaded {len(chunks)} chunk paths from {CHUNKS_FILE}\")\n    else:\n        # First time: actually split\n        chunks = split_into_chunks(VIDEO_IN, CLIPS_DIR, CHUNK_RANGES)\n        with open(CHUNKS_FILE, \"w\") as f:\n            for p in chunks:\n                f.write(p + \"\\n\")\n        print(f\"[SPLIT] Saved {len(chunks)} chunk paths to {CHUNKS_FILE}\")\n\n    task_q = multiprocessing.Queue()\n    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n    p.start()\n\n    for c in chunks:\n        task_q.put((\"management\", c))\n        # task_q.put((\"violation\",  c))\n    task_q.put(None)\n\n    # threading.Thread(target=run_simulation_loop,\n    #                  args=(RECO_DIR,VIOL_DIR), daemon=True).start()\n    # threading.Thread(target=lambda: app.run(port=8888, host=\"0.0.0.0\"),\n    #                  daemon=True).start()\n\n    p.join()\n    print(\"All GPU tasks done.\")","metadata":{"trusted":true,"_kg_hide-output":false},"outputs":[],"execution_count":null}]}