{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjF-8ijbvpBm",
        "outputId": "729ef59f-ba8b-4535-9f18-671091ef7a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Colab Cell 1: install & mount\n",
        "!pip install ultralytics flask pymongo --quiet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Path to your full video\n",
        "VIDEO_IN = \"/content/drive/MyDrive/Dataset_for_graduation/videos/VideoInputStream.mp4\"\n",
        "\n",
        "# Your chunk frame ranges\n",
        "CHUNK_RANGES = [\n",
        "    (0,   2999),\n",
        "    (3000,5999),\n",
        "    (6000,8999),\n",
        "    (9000,11999),\n",
        "    (12000,14999),\n",
        "    (15000,18049),\n",
        "    (18050,20999),\n",
        "    (21000,24099),\n",
        "    (24100,26851)\n",
        "]\n",
        "\n",
        "# Open video just to read its FPS\n",
        "cap = cv2.VideoCapture(VIDEO_IN)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "cap.release()\n",
        "\n",
        "print(f\"Video FPS = {fps:.2f}\\n\")\n",
        "\n",
        "for idx, (start, end) in enumerate(CHUNK_RANGES):\n",
        "    # number of frames in this chunk (inclusive)\n",
        "    n_frames = end - start + 1\n",
        "    # duration in seconds\n",
        "    duration_sec = n_frames / fps\n",
        "    print(f\"Chunk {idx}: frames [{start:>5}–{end:<5}] → {n_frames} frames → {duration_sec:.2f} s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke_h-3zhBT_-",
        "outputId": "170d16ca-841d-4c3c-cb3b-ed688e19aedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video FPS = 30.00\n",
            "\n",
            "Chunk 0: frames [    0–2999 ] → 3000 frames → 100.00 s\n",
            "Chunk 1: frames [ 3000–5999 ] → 3000 frames → 100.00 s\n",
            "Chunk 2: frames [ 6000–8999 ] → 3000 frames → 100.00 s\n",
            "Chunk 3: frames [ 9000–11999] → 3000 frames → 100.00 s\n",
            "Chunk 4: frames [12000–14999] → 3000 frames → 100.00 s\n",
            "Chunk 5: frames [15000–18049] → 3050 frames → 101.67 s\n",
            "Chunk 6: frames [18050–20999] → 2950 frames → 98.33 s\n",
            "Chunk 7: frames [21000–24099] → 3100 frames → 103.33 s\n",
            "Chunk 8: frames [24100–26851] → 2752 frames → 91.73 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# 1) Imports & Config\n",
        "# ----------------------------------------\n",
        "import gc, cv2, os, csv, json, threading, multiprocessing, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from pymongo import MongoClient\n",
        "import base64\n",
        "from flask import Flask, jsonify\n",
        "import glob\n",
        "import logging\n",
        "import shutil\n",
        "import time\n",
        "import socket\n",
        "\n",
        "# Paths & constants\n",
        "VIDEO_IN       = \"/content/drive/MyDrive/Dataset_for_graduation/videos/VideoInputStream.mp4\"\n",
        "POLY_CSV       = \"/content/drive/MyDrive/Dataset_for_graduation/videos/polygons.csv\"\n",
        "MODEL_PT       = \"/content/drive/MyDrive/Dataset_for_graduation/videos/best.pt\"\n",
        "DRIVE_CHUNKS   = \"/content/drive/MyDrive/Dataset_for_graduation/videos/chunks\"\n",
        "LOCAL_CHUNKS   = \"/content/local_chunks\"\n",
        "CLIPS_DIR      = \"clips\"\n",
        "RECO_DIR       = Path(\"recommendations\")\n",
        "VIOL_DIR       = Path(\"violations\") # could save to database from violation's code\n",
        "OUT_DIR        = Path(\"outputs_video\") # Contains all the annotated chunks with frames for each annotated chunk\n",
        "LIGHT_DIR      = OUT_DIR/\"original_lights\"\n",
        "COUNTS_DIR     = OUT_DIR/\"counts\"\n",
        "BEST_FRAME_DIR = Path(\"best_frames\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# Setup a basic logger\n",
        "# ----------------------------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "# MongoDB setup\n",
        "\n",
        "# old\n",
        "# MONGO_URI = os.getenv(\n",
        "#     \"MONGO_URI\",\n",
        "#     \"mongodb+srv://nafe:0597785625nafe@coffeeshop.s8duwhp.mongodb.net/?retryWrites=true&w=majority\"\n",
        "# )\n",
        "\n",
        "# mongodb+srv://abubakernafe1:0597785625nafe@trafficmanagement.r28vab3.mongodb.net/\n",
        "# mongodb+srv://abubakernafe1:<db_password>@trafficmanagement.r28vab3.mongodb.net/?retryWrites=true&w=majority&appName=trafficmanagement\n",
        "\n",
        "# new\n",
        "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb+srv://abubakernafe1:0597785625nafe@trafficmanagement.r28vab3.mongodb.net/?retryWrites=true&w=majority\")\n",
        "DB_NAME  = os.getenv(\"DB_NAME\", \"trafficmanagement\")\n",
        "COL_NAME = os.getenv(\"COL_NAME\", \"records\")\n",
        "\n",
        "mongo_client = MongoClient(MONGO_URI)\n",
        "mongo_db     = mongo_client[DB_NAME]\n",
        "records      = mongo_db[COL_NAME]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Chunk boundaries\n",
        "CHUNK_RANGES = [\n",
        "    (0,2999),(3000,5999),(6000,8999),(9000,11999),\n",
        "    (12000,14999),(15000,18049),(18050,20999),\n",
        "    (21000,24099),(24100,10**9)\n",
        "]\n",
        "\n",
        "# Chunk boundaries (global frame numbers)\n",
        "# CHUNK_RANGES = [(0,   2999), (3000,5999)]\n",
        "\n",
        "# Ensure output dirs exist\n",
        "for d in (CLIPS_DIR, RECO_DIR, VIOL_DIR, OUT_DIR, LIGHT_DIR, COUNTS_DIR, BEST_FRAME_DIR):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "for tid in (\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"):\n",
        "    (BEST_FRAME_DIR/tid).mkdir(exist_ok=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Splitting Logic\n",
        "# ----------------------------------------\n",
        "def split_into_chunks(video_path, output_dir, chunk_ranges):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    paths = []\n",
        "    for idx, (start, end) in enumerate(chunk_ranges):\n",
        "        logger.info(f\"[SPLIT] Creating chunk {idx} frames {start}→{end}\")\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "        out_path = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n",
        "        writer = cv2.VideoWriter(out_path,\n",
        "                                 cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                                 fps, (w, h))\n",
        "        for _ in range(end - start + 1):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            writer.write(frame)\n",
        "        writer.release()\n",
        "        if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
        "            paths.append(out_path)\n",
        "            logger.info(f\"[SPLIT] Saved {out_path}\")\n",
        "        else:\n",
        "            logger.info(f\"[SPLIT] Removed empty {out_path}\")\n",
        "            os.remove(out_path)\n",
        "    cap.release()\n",
        "    return paths\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) BEFORE anything else, copy chunks locally\n",
        "# ----------------------------------------\n",
        "def prepare_local_chunks():\n",
        "    logger.info(\"Preparing local chunk directory…\")\n",
        "    os.makedirs(LOCAL_CHUNKS, exist_ok=True)\n",
        "    drive_files = sorted(glob.glob(f\"{DRIVE_CHUNKS}/chunk_*.mp4\"))\n",
        "    logger.info(f\"Found {len(drive_files)} chunks on Drive.\")\n",
        "    t0 = time.time()\n",
        "    for src in drive_files:\n",
        "        dst = os.path.join(LOCAL_CHUNKS, Path(src).name)\n",
        "        logger.info(f\"Copying {src} → {dst}\")\n",
        "        shutil.copy2(src, dst)\n",
        "    dt = time.time() - t0\n",
        "    logger.info(f\"Copied {len(drive_files)} files in {dt:.1f}s.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) data for management (includes: polygons for both intersections & traffic lights, colours, priority and weights)\n",
        "# ----------------------------------------\n",
        "poly_by_frame = {}\n",
        "with open(POLY_CSV, newline=\"\") as f:\n",
        "    for row in csv.DictReader(f):\n",
        "        fid, pid = int(row[\"frame\"]), row[\"id\"]\n",
        "        pts = np.array([[float(row[f\"x{i}\"]), float(row[f\"y{i}\"])]\n",
        "                        for i in range(1,5)], np.float32)\n",
        "        poly_by_frame.setdefault(fid, []).append((pid, pts))\n",
        "\n",
        "traffic_light_polygons = [\n",
        "    (\"ID-1\",  15,  83, 40,130),\n",
        "    (\"ID-2\", 105,  83, 40,130),\n",
        "    (\"ID-3\", 180,  83, 40,130),\n",
        "    (\"ID-4\", 270,  83, 40,130),\n",
        "]\n",
        "COLOURS   = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\n",
        "TL_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\n",
        "PRIORITY  = {\"red\":3,\"yellow\":2,\"green\":1}\n",
        "WEIGHTS   = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) Traffic Management\n",
        "# ----------------------------------------\n",
        "def run_traffic_management(chunk_path: str, model: YOLO) -> None:\n",
        "    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n",
        "    start, end = CHUNK_RANGES[chunk_id]\n",
        "\n",
        "    logger.info(f\"[Chunk {chunk_id}] START processing {chunk_path}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    cap = cv2.VideoCapture(chunk_path)\n",
        "    w,h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    writer = cv2.VideoWriter(\n",
        "        str(OUT_DIR/f\"annotated_chunk_{chunk_id}.mp4\"),\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h)\n",
        "    )\n",
        "\n",
        "    best_car_counts = {tid:-1 for tid,*_ in traffic_light_polygons}\n",
        "    prev_tl_state   = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n",
        "\n",
        "    local_idx, last_polys = 1, []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # convert to global frame index\n",
        "        global_idx = start + local_idx - 1\n",
        "        if local_idx % 500 == 1:\n",
        "            logger.info(f\"[MGMT][Chunk {chunk_id}] Local {local_idx} → Global {global_idx}\")\n",
        "\n",
        "        # lookup or reuse static polygons\n",
        "        if global_idx in poly_by_frame:\n",
        "            polys = [(pid, poly.astype(np.int32))\n",
        "                     for pid, poly in poly_by_frame[global_idx]]\n",
        "            last_polys = polys\n",
        "        else:\n",
        "            polys = last_polys\n",
        "\n",
        "        # YOLO inference + overlay\n",
        "        res   = model(frame, conf=0.20, verbose=False, show_labels=False)[0]\n",
        "        boxes = res.boxes\n",
        "        frame = res.plot(img=frame, labels=True, line_width=1)\n",
        "\n",
        "        # dump YOLO txt\n",
        "        yolo_lines = []\n",
        "        for b in boxes:\n",
        "            cls = int(b.cls[0])\n",
        "            x1,y1,x2,y2 = b.xyxy[0]\n",
        "            cx,cy = (x1+x2)/2, (y1+y2)/2\n",
        "            bw,bh = (x2-x1),(y2-y1)\n",
        "            yolo_lines.append(f\"{cls} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\")\n",
        "        (OUT_DIR/f\"chunk{chunk_id}_frame_{local_idx:06d}.yolo.txt\")\\\n",
        "          .write_text(\"\\n\".join(yolo_lines))\n",
        "\n",
        "        # count cars in each polygon\n",
        "        counts = {pid:0 for pid,_ in polys}\n",
        "        for ln in yolo_lines:\n",
        "            cls,cxn,cyn,*_ = ln.split()\n",
        "            if int(cls)!=0: # if object is not car, pass\n",
        "                continue\n",
        "            cx,cy = float(cxn)*w, float(cyn)*h\n",
        "            # if car's center (x,y) is in the range of the polygon, add 1 to the pid's car's count\n",
        "            for pid,poly in polys:\n",
        "                if cv2.pointPolygonTest(poly,(cx,cy),False)>=0:\n",
        "                    counts[pid] += 1\n",
        "                    break\n",
        "\n",
        "        # detect TL states\n",
        "        tl_state = {tid:\"unknown\" for tid,*_ in traffic_light_polygons}\n",
        "        for ln in yolo_lines:\n",
        "            cls,cxn,cyn,*_ = ln.split()\n",
        "            cid = int(cls)\n",
        "            if cid not in (1,2,3):\n",
        "                continue\n",
        "            colour = {1:\"green\",2:\"red\",3:\"yellow\"}[cid]\n",
        "            cx,cy = float(cxn)*w, float(cyn)*h\n",
        "            for tid,px,py,pw,ph in traffic_light_polygons:\n",
        "                if px<=cx<=px+pw and py<=cy<=py+ph:\n",
        "                    if PRIORITY[colour] > PRIORITY.get(tl_state[tid],0):\n",
        "                        tl_state[tid] = colour\n",
        "                    break\n",
        "\n",
        "        # overlay counts & TL panels\n",
        "        for i,(pid,poly) in enumerate(polys):\n",
        "            col = COLOURS[i%len(COLOURS)]\n",
        "            cv2.polylines(frame,[poly],True,col,2)\n",
        "            cv2.putText(frame,f\"{pid}:{counts[pid]}\",\n",
        "                        (w-300,40+i*40),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n",
        "        for tid,px,py,pw,ph in traffic_light_polygons:\n",
        "            col = TL_COLOUR[tl_state[tid]]\n",
        "            cv2.rectangle(frame,(px,py),(px+pw,py+ph),col,2)\n",
        "            cv2.putText(frame,f\"{tid}:{tl_state[tid]}\",\n",
        "                        (px+2,py-6),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,0.5,col,1)\n",
        "\n",
        "        writer.write(frame)\n",
        "\n",
        "        # On a YELLOW→GREEN transition, save the frames with yellow state for the traffic light, to determine the best frame for the recommendation\n",
        "        for tid in tl_state:\n",
        "            if prev_tl_state[tid]==\"yellow\" and tl_state[tid]==\"green\":\n",
        "                logger.info(f\"[MGMT][Chunk {chunk_id}] {tid} Y→G @local {local_idx}, saving\")\n",
        "                (COUNTS_DIR/f\"chunk{chunk_id}_frame_{local_idx:06d}.txt\")\\\n",
        "                  .write_text(json.dumps(counts))\n",
        "                (LIGHT_DIR/f\"chunk{chunk_id}_frame_{local_idx:06d}.json\")\\\n",
        "                  .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n",
        "                c = counts.get(tid,0)\n",
        "                if c>best_car_counts[tid]:\n",
        "                    best_car_counts[tid] = c\n",
        "                    sd = BEST_FRAME_DIR/tid\n",
        "                    cv2.imwrite(str(sd/f\"chunk{chunk_id}_best_frame.jpg\"),frame)\n",
        "                    (sd/f\"chunk{chunk_id}_best_frame.json\")\\\n",
        "                      .write_text(json.dumps({\"cars\":counts,\"lights\":tl_state}))\n",
        "                    (sd/f\"chunk{chunk_id}_best_frame.txt\")\\\n",
        "                      .write_text(json.dumps(counts))\n",
        "                    logger.info(f\"[MGMT][Chunk {chunk_id}] New best {tid}: {c} cars\")\n",
        "\n",
        "        prev_tl_state = tl_state.copy()\n",
        "        local_idx    += 1\n",
        "\n",
        "\n",
        "    # at the very end:\n",
        "    dt = time.time() - t0\n",
        "    logger.info(f\"[Chunk {chunk_id}] DONE in {dt:.1f}s\")\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    gc.collect()\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # 4.x) Recommendations (bump all counts by 2)\n",
        "    # ----------------------------------------\n",
        "    logger.info(f\"[MGMT][Chunk {chunk_id}] Generating recommendation JSON\")\n",
        "    best_data = {}\n",
        "    for tid, *_ in traffic_light_polygons:\n",
        "        p = BEST_FRAME_DIR/tid/f\"chunk{chunk_id}_best_frame.json\"\n",
        "        best_data[tid] = (\n",
        "            json.loads(p.read_text())\n",
        "            if p.exists()\n",
        "            else {\"cars\": {tid: 0}, \"lights\": {tid: \"unknown\"}}\n",
        "        )\n",
        "\n",
        "    # compute weighted scores, for each traffic ID, store num of cars in each one of them\n",
        "    weighted   = {tid: best_data[tid][\"cars\"].get(tid, 0) * WEIGHTS[tid] for tid in best_data}\n",
        "    candidates = list(weighted) # conv to list\n",
        "    recs       = []\n",
        "\n",
        "    for current in (\"ID-4\", \"ID-1\", \"ID-3\",\"ID-2\"): # we stick to this order, because ID-2 turns yellow very late in the chunk\n",
        "        rec  = max(candidates, key=lambda t: weighted[t]) # IDK\n",
        "        data = best_data[rec]\n",
        "\n",
        "        # bump **all** counts by 2\n",
        "        all_counts = {tid: cnt + 2 for tid, cnt in data[\"cars\"].items()}\n",
        "\n",
        "        # copy light states and force the recommended one to yellow\n",
        "        all_states = data[\"lights\"].copy()\n",
        "        all_states[rec] = \"yellow\"\n",
        "\n",
        "        recs.append({\n",
        "            \"current\":      current,\n",
        "            \"recommended\":  rec,\n",
        "            \"duration_sec\": (all_counts[rec] * 2 + 2),\n",
        "            \"all_counts\":   all_counts,\n",
        "            \"all_states\":   all_states\n",
        "        })\n",
        "        candidates.remove(rec)\n",
        "\n",
        "    # write out JSON\n",
        "    (RECO_DIR/f\"chunk_{chunk_id}_recommendations.json\") \\\n",
        "      .write_text(json.dumps(recs, indent=2))\n",
        "    logger.info(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n",
        "\n",
        "    # save into mongoDB\n",
        "\n",
        "    # 1) record the path to the raw chunk on disk (for future, I'll be saving the video link)\n",
        "    video_path = str(Path(CLIPS_DIR) / f\"chunk_{chunk_id}.mp4\")\n",
        "\n",
        "    # 2) encode best-frame images into base64\n",
        "    best_frames_list = []\n",
        "    for tid, *_ in traffic_light_polygons:\n",
        "        img_path = BEST_FRAME_DIR / tid / f\"chunk{chunk_id}_best_frame.jpg\"\n",
        "        if img_path.exists():\n",
        "            img_b64 = base64.b64encode(img_path.read_bytes()).decode(\"utf-8\")\n",
        "        else:\n",
        "            img_b64 = None\n",
        "        best_frames_list.append({\n",
        "            \"id\":    tid,\n",
        "            \"image\": img_b64\n",
        "        })\n",
        "\n",
        "    # 3) upsert document in MongoDB\n",
        "    record = {\n",
        "        \"chunk\":           chunk_id,\n",
        "        \"video_path\":   video_path,\n",
        "        \"recommendations\": recs,\n",
        "        \"best_frames\":     best_frames_list\n",
        "    }\n",
        "    records.update_one(\n",
        "        {\"chunk\": chunk_id},\n",
        "        {\"$set\": record},\n",
        "        upsert=True\n",
        "    )\n",
        "    logger.info(f\"wrote {records} to records collection on mognodb\")\n",
        "    logger.info(\"Done with management!\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) (Your violation detection follows…)\n",
        "# ----------------------------------------\n",
        "def run_violation_detection(chunk_path: str, _model=None) -> None:\n",
        "    # … unchanged …\n",
        "    pass\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) GPU Worker & Orchestrator\n",
        "# ----------------------------------------\n",
        "def gpu_worker(q):\n",
        "    model = YOLO(MODEL_PT)\n",
        "    while True:\n",
        "        task = q.get()\n",
        "        if task is None: break\n",
        "        kind, chunk = task\n",
        "        if kind==\"management\":\n",
        "            run_traffic_management(chunk, model)\n",
        "        # else:\n",
        "        #     print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n",
        "            # run_violation_detection(chunk, model)\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route(\"/reco/<chunk_id>\")\n",
        "def get_reco(chunk_id):\n",
        "    return jsonify(json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\")))\n",
        "\n",
        "def run_simulation_loop(reco_dir, viol_dir):\n",
        "    while True: pass\n",
        "\n",
        "def find_free_port():\n",
        "    s = socket.socket()\n",
        "    s.bind(('', 0))  # Let OS assign a free port\n",
        "    port = s.getsockname()[1]\n",
        "    s.close()\n",
        "    return port\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    # chunks1 = split_into_chunks(VIDEO_IN, CLIPS_DIR, CHUNK_RANGES)\n",
        "    # chunks = chunks1\n",
        "\n",
        "    # we already have pre-split files in Drive, so skip split_into_chunks()\n",
        "    # and just read them directly:\n",
        "\n",
        "    # 1) copy once at startup\n",
        "    prepare_local_chunks()\n",
        "\n",
        "    # 2) glob from local instead of Drive\n",
        "    chunks = sorted(\n",
        "        glob.glob(f\"{LOCAL_CHUNKS}/chunk_*.mp4\"),\n",
        "        key=lambda p: int(Path(p).stem.split(\"_\")[-1])\n",
        "    )\n",
        "    logger.info(f\"Processing {len(chunks)} chunks from local disk.\")\n",
        "\n",
        "    task_q = multiprocessing.Queue()\n",
        "    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n",
        "    p.start()\n",
        "\n",
        "    for c in chunks:\n",
        "        logger.info(f\"Enqueuing management task for {c}\")\n",
        "        task_q.put((\"management\", c))\n",
        "        # task_q.put((\"violation\",  c))\n",
        "    task_q.put(None)\n",
        "\n",
        "    PORT = find_free_port()\n",
        "    threading.Thread(target=run_simulation_loop, args=(RECO_DIR,VIOL_DIR), daemon=True).start()\n",
        "    threading.Thread(target=lambda: app.run(port=PORT, host=\"0.0.0.0\"), daemon=True).start()\n",
        "\n",
        "    p.join()\n",
        "    logger.info(\"All GPU tasks done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr87unmZvrD4",
        "outputId": "e4e6ebf6-418e-4be5-a718-76d03682a2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "02:11:18 INFO     Preparing local chunk directory…\n",
            "02:11:18 INFO     Found 9 chunks on Drive.\n",
            "02:11:18 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_0.mp4 → /content/local_chunks/chunk_0.mp4\n",
            "02:11:19 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_1.mp4 → /content/local_chunks/chunk_1.mp4\n",
            "02:11:20 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_2.mp4 → /content/local_chunks/chunk_2.mp4\n",
            "02:11:22 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_3.mp4 → /content/local_chunks/chunk_3.mp4\n",
            "02:11:23 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_4.mp4 → /content/local_chunks/chunk_4.mp4\n",
            "02:11:24 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_5.mp4 → /content/local_chunks/chunk_5.mp4\n",
            "02:11:25 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_6.mp4 → /content/local_chunks/chunk_6.mp4\n",
            "02:11:26 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_7.mp4 → /content/local_chunks/chunk_7.mp4\n",
            "02:11:26 INFO     Copying /content/drive/MyDrive/Dataset_for_graduation/videos/chunks/chunk_8.mp4 → /content/local_chunks/chunk_8.mp4\n",
            "02:11:29 INFO     Copied 9 files in 11.5s.\n",
            "02:11:29 INFO     Processing 9 chunks from local disk.\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_0.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_1.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_2.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_3.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_4.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_5.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_6.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_7.mp4\n",
            "02:11:29 INFO     Enqueuing management task for /content/local_chunks/chunk_8.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "02:11:30 INFO     \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:47595\n",
            " * Running on http://172.28.0.12:47595\n",
            "02:11:30 INFO     \u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "02:11:31 INFO     [Chunk 0] START processing /content/local_chunks/chunk_0.mp4\n",
            "02:11:31 INFO     [MGMT][Chunk 0] Local 1 → Global 0\n",
            "02:12:29 INFO     [MGMT][Chunk 0] Local 501 → Global 500\n",
            "02:12:31 INFO     [MGMT][Chunk 0] ID-4 Y→G @local 510, saving\n",
            "02:12:31 INFO     [MGMT][Chunk 0] New best ID-4: 3 cars\n",
            "02:13:20 INFO     [MGMT][Chunk 0] Local 1001 → Global 1000\n",
            "02:13:22 INFO     [MGMT][Chunk 0] ID-1 Y→G @local 1020, saving\n",
            "02:13:22 INFO     [MGMT][Chunk 0] New best ID-1: 10 cars\n",
            "02:14:12 INFO     [MGMT][Chunk 0] Local 1501 → Global 1500\n",
            "02:14:24 INFO     [MGMT][Chunk 0] ID-3 Y→G @local 1620, saving\n",
            "02:14:24 INFO     [MGMT][Chunk 0] New best ID-3: 2 cars\n",
            "02:15:02 INFO     [MGMT][Chunk 0] Local 2001 → Global 2000\n",
            "02:15:51 INFO     [MGMT][Chunk 0] Local 2501 → Global 2500\n",
            "02:16:38 INFO     [MGMT][Chunk 0] ID-2 Y→G @local 3000, saving\n",
            "02:16:38 INFO     [MGMT][Chunk 0] New best ID-2: 0 cars\n",
            "02:16:38 INFO     [Chunk 0] DONE in 306.6s\n",
            "02:16:38 INFO     [MGMT][Chunk 0] Generating recommendation JSON\n",
            "02:16:38 INFO     [MGMT][Chunk 0] Recommendations saved\n",
            "02:16:41 INFO     Done with management!\n",
            "02:16:41 INFO     [Chunk 1] START processing /content/local_chunks/chunk_1.mp4\n",
            "02:16:41 INFO     [MGMT][Chunk 1] Local 1 → Global 3000\n",
            "02:17:30 INFO     [MGMT][Chunk 1] Local 501 → Global 3500\n",
            "02:17:30 INFO     [MGMT][Chunk 1] ID-4 Y→G @local 510, saving\n",
            "02:17:30 INFO     [MGMT][Chunk 1] New best ID-4: 2 cars\n",
            "02:18:18 INFO     [MGMT][Chunk 1] Local 1001 → Global 4000\n",
            "02:18:21 INFO     [MGMT][Chunk 1] ID-1 Y→G @local 1020, saving\n",
            "02:18:21 INFO     [MGMT][Chunk 1] New best ID-1: 14 cars\n",
            "02:19:07 INFO     [MGMT][Chunk 1] Local 1501 → Global 4500\n",
            "02:19:19 INFO     [MGMT][Chunk 1] ID-3 Y→G @local 1620, saving\n",
            "02:19:19 INFO     [MGMT][Chunk 1] New best ID-3: 4 cars\n",
            "02:19:58 INFO     [MGMT][Chunk 1] Local 2001 → Global 5000\n",
            "02:20:47 INFO     [MGMT][Chunk 1] Local 2501 → Global 5500\n",
            "02:21:37 INFO     [MGMT][Chunk 1] ID-2 Y→G @local 3000, saving\n",
            "02:21:37 INFO     [MGMT][Chunk 1] New best ID-2: 0 cars\n",
            "02:21:37 INFO     [Chunk 1] DONE in 295.3s\n",
            "02:21:37 INFO     [MGMT][Chunk 1] Generating recommendation JSON\n",
            "02:21:37 INFO     [MGMT][Chunk 1] Recommendations saved\n",
            "02:21:38 INFO     Done with management!\n",
            "02:21:38 INFO     [Chunk 2] START processing /content/local_chunks/chunk_2.mp4\n",
            "02:21:38 INFO     [MGMT][Chunk 2] Local 1 → Global 6000\n",
            "02:22:26 INFO     [MGMT][Chunk 2] Local 501 → Global 6500\n",
            "02:22:27 INFO     [MGMT][Chunk 2] ID-4 Y→G @local 510, saving\n",
            "02:22:27 INFO     [MGMT][Chunk 2] New best ID-4: 5 cars\n",
            "02:23:17 INFO     [MGMT][Chunk 2] Local 1001 → Global 7000\n",
            "02:23:19 INFO     [MGMT][Chunk 2] ID-1 Y→G @local 1020, saving\n",
            "02:23:19 INFO     [MGMT][Chunk 2] New best ID-1: 15 cars\n",
            "02:24:07 INFO     [MGMT][Chunk 2] Local 1501 → Global 7500\n",
            "02:24:19 INFO     [MGMT][Chunk 2] ID-3 Y→G @local 1620, saving\n",
            "02:24:19 INFO     [MGMT][Chunk 2] New best ID-3: 2 cars\n",
            "02:25:00 INFO     [MGMT][Chunk 2] Local 2001 → Global 8000\n",
            "02:25:52 INFO     [MGMT][Chunk 2] Local 2501 → Global 8500\n",
            "02:26:45 INFO     [MGMT][Chunk 2] ID-2 Y→G @local 3000, saving\n",
            "02:26:45 INFO     [MGMT][Chunk 2] New best ID-2: 4 cars\n",
            "02:26:45 INFO     [Chunk 2] DONE in 307.6s\n",
            "02:26:46 INFO     [MGMT][Chunk 2] Generating recommendation JSON\n",
            "02:26:46 INFO     [MGMT][Chunk 2] Recommendations saved\n",
            "02:26:47 INFO     Done with management!\n",
            "02:26:47 INFO     [Chunk 3] START processing /content/local_chunks/chunk_3.mp4\n",
            "02:26:47 INFO     [MGMT][Chunk 3] Local 1 → Global 9000\n",
            "02:27:37 INFO     [MGMT][Chunk 3] Local 501 → Global 9500\n",
            "02:27:37 INFO     [MGMT][Chunk 3] ID-4 Y→G @local 510, saving\n",
            "02:27:37 INFO     [MGMT][Chunk 3] New best ID-4: 3 cars\n",
            "02:28:28 INFO     [MGMT][Chunk 3] Local 1001 → Global 10000\n",
            "02:28:30 INFO     [MGMT][Chunk 3] ID-1 Y→G @local 1020, saving\n",
            "02:28:30 INFO     [MGMT][Chunk 3] New best ID-1: 20 cars\n",
            "02:29:19 INFO     [MGMT][Chunk 3] Local 1501 → Global 10500\n",
            "02:30:03 INFO     [MGMT][Chunk 3] ID-3 Y→G @local 1900, saving\n",
            "02:30:03 INFO     [MGMT][Chunk 3] New best ID-3: 5 cars\n",
            "02:30:14 INFO     [MGMT][Chunk 3] Local 2001 → Global 11000\n",
            "02:31:08 INFO     [MGMT][Chunk 3] Local 2501 → Global 11500\n",
            "02:32:03 INFO     [MGMT][Chunk 3] ID-2 Y→G @local 3000, saving\n",
            "02:32:03 INFO     [Chunk 3] DONE in 316.0s\n",
            "02:32:03 INFO     [MGMT][Chunk 3] New best ID-2: 5 cars\n",
            "02:32:03 INFO     [MGMT][Chunk 3] Generating recommendation JSON\n",
            "02:32:03 INFO     [MGMT][Chunk 3] Recommendations saved\n",
            "02:32:04 INFO     Done with management!\n",
            "02:32:04 INFO     [Chunk 4] START processing /content/local_chunks/chunk_4.mp4\n",
            "02:32:04 INFO     [MGMT][Chunk 4] Local 1 → Global 12000\n",
            "02:32:59 INFO     [MGMT][Chunk 4] Local 501 → Global 12500\n",
            "02:33:02 INFO     [MGMT][Chunk 4] ID-4 Y→G @local 540, saving\n",
            "02:33:02 INFO     [MGMT][Chunk 4] New best ID-4: 4 cars\n",
            "02:33:51 INFO     [MGMT][Chunk 4] Local 1001 → Global 13000\n",
            "02:34:02 INFO     [MGMT][Chunk 4] ID-1 Y→G @local 1110, saving\n",
            "02:34:02 INFO     [MGMT][Chunk 4] New best ID-1: 23 cars\n",
            "02:34:43 INFO     [MGMT][Chunk 4] Local 1501 → Global 13500\n",
            "02:35:06 INFO     [MGMT][Chunk 4] ID-3 Y→G @local 1710, saving\n",
            "02:35:06 INFO     [MGMT][Chunk 4] New best ID-3: 2 cars\n",
            "02:35:38 INFO     [MGMT][Chunk 4] Local 2001 → Global 14000\n",
            "02:36:30 INFO     [MGMT][Chunk 4] Local 2501 → Global 14500\n",
            "02:37:20 INFO     [MGMT][Chunk 4] ID-2 Y→G @local 3000, saving\n",
            "02:37:20 INFO     [Chunk 4] DONE in 316.5s\n",
            "02:37:20 INFO     [MGMT][Chunk 4] New best ID-2: 3 cars\n",
            "02:37:21 INFO     [MGMT][Chunk 4] Generating recommendation JSON\n",
            "02:37:21 INFO     [MGMT][Chunk 4] Recommendations saved\n",
            "02:37:22 INFO     Done with management!\n",
            "02:37:22 INFO     [Chunk 5] START processing /content/local_chunks/chunk_5.mp4\n",
            "02:37:22 INFO     [MGMT][Chunk 5] Local 1 → Global 15000\n",
            "02:38:15 INFO     [MGMT][Chunk 5] Local 501 → Global 15500\n",
            "02:38:28 INFO     [MGMT][Chunk 5] ID-4 Y→G @local 630, saving\n",
            "02:38:28 INFO     [MGMT][Chunk 5] New best ID-4: 3 cars\n",
            "02:39:07 INFO     [MGMT][Chunk 5] Local 1001 → Global 16000\n",
            "02:39:27 INFO     [MGMT][Chunk 5] ID-1 Y→G @local 1200, saving\n",
            "02:39:27 INFO     [MGMT][Chunk 5] New best ID-1: 23 cars\n",
            "02:39:57 INFO     [MGMT][Chunk 5] Local 1501 → Global 16500\n",
            "02:40:29 INFO     [MGMT][Chunk 5] ID-3 Y→G @local 1800, saving\n",
            "02:40:29 INFO     [MGMT][Chunk 5] New best ID-3: 2 cars\n",
            "02:40:50 INFO     [MGMT][Chunk 5] Local 2001 → Global 17000\n",
            "02:41:44 INFO     [MGMT][Chunk 5] Local 2501 → Global 17500\n",
            "02:42:34 INFO     [MGMT][Chunk 5] Local 3001 → Global 18000\n",
            "02:42:40 INFO     [MGMT][Chunk 5] ID-2 Y→G @local 3050, saving\n",
            "02:42:40 INFO     [MGMT][Chunk 5] New best ID-2: 3 cars\n",
            "02:42:40 INFO     [Chunk 5] DONE in 318.2s\n",
            "02:42:41 INFO     [MGMT][Chunk 5] Generating recommendation JSON\n",
            "02:42:41 INFO     [MGMT][Chunk 5] Recommendations saved\n",
            "02:42:42 INFO     Done with management!\n",
            "02:42:42 INFO     [Chunk 6] START processing /content/local_chunks/chunk_6.mp4\n",
            "02:42:42 INFO     [MGMT][Chunk 6] Local 1 → Global 18050\n",
            "02:43:37 INFO     [MGMT][Chunk 6] Local 501 → Global 18550\n",
            "02:43:42 INFO     [MGMT][Chunk 6] ID-4 Y→G @local 560, saving\n",
            "02:43:42 INFO     [MGMT][Chunk 6] New best ID-4: 7 cars\n",
            "02:44:33 INFO     [MGMT][Chunk 6] Local 1001 → Global 19050\n",
            "02:44:57 INFO     [MGMT][Chunk 6] ID-1 Y→G @local 1240, saving\n",
            "02:44:57 INFO     [MGMT][Chunk 6] New best ID-1: 23 cars\n",
            "02:45:27 INFO     [MGMT][Chunk 6] Local 1501 → Global 19550\n",
            "02:46:04 INFO     [MGMT][Chunk 6] ID-3 Y→G @local 1840, saving\n",
            "02:46:04 INFO     [MGMT][Chunk 6] New best ID-3: 4 cars\n",
            "02:46:21 INFO     [MGMT][Chunk 6] Local 2001 → Global 20050\n",
            "02:47:18 INFO     [MGMT][Chunk 6] Local 2501 → Global 20550\n",
            "02:48:08 INFO     [MGMT][Chunk 6] ID-2 Y→G @local 2950, saving\n",
            "02:48:08 INFO     [MGMT][Chunk 6] New best ID-2: 2 cars\n",
            "02:48:08 INFO     [Chunk 6] DONE in 326.3s\n",
            "02:48:09 INFO     [MGMT][Chunk 6] Generating recommendation JSON\n",
            "02:48:09 INFO     [MGMT][Chunk 6] Recommendations saved\n",
            "02:48:10 INFO     Done with management!\n",
            "02:48:10 INFO     [Chunk 7] START processing /content/local_chunks/chunk_7.mp4\n",
            "02:48:10 INFO     [MGMT][Chunk 7] Local 1 → Global 21000\n",
            "02:49:06 INFO     [MGMT][Chunk 7] Local 501 → Global 21500\n",
            "02:49:22 INFO     [MGMT][Chunk 7] ID-4 Y→G @local 650, saving\n",
            "02:49:22 INFO     [MGMT][Chunk 7] New best ID-4: 6 cars\n",
            "02:50:03 INFO     [MGMT][Chunk 7] Local 1001 → Global 22000\n",
            "02:50:24 INFO     [MGMT][Chunk 7] ID-1 Y→G @local 1200, saving\n",
            "02:50:24 INFO     [MGMT][Chunk 7] New best ID-1: 24 cars\n",
            "02:51:00 INFO     [MGMT][Chunk 7] Local 1501 → Global 22500\n",
            "02:51:53 INFO     [MGMT][Chunk 7] ID-3 Y→G @local 1980, saving\n",
            "02:51:53 INFO     [MGMT][Chunk 7] New best ID-3: 8 cars\n",
            "02:51:56 INFO     [MGMT][Chunk 7] Local 2001 → Global 23000\n",
            "02:52:52 INFO     [MGMT][Chunk 7] Local 2501 → Global 23500\n",
            "02:53:47 INFO     [MGMT][Chunk 7] Local 3001 → Global 24000\n",
            "02:53:52 INFO     [MGMT][Chunk 7] ID-2 Y→G @local 3050, saving\n",
            "02:53:52 INFO     [MGMT][Chunk 7] New best ID-2: 3 cars\n",
            "02:53:56 INFO     [Chunk 7] DONE in 346.3s\n",
            "02:53:57 INFO     [MGMT][Chunk 7] Generating recommendation JSON\n",
            "02:53:57 INFO     [MGMT][Chunk 7] Recommendations saved\n",
            "02:53:58 INFO     Done with management!\n",
            "02:53:58 INFO     [Chunk 8] START processing /content/local_chunks/chunk_8.mp4\n",
            "02:53:58 INFO     [MGMT][Chunk 8] Local 1 → Global 24100\n",
            "02:54:55 INFO     [MGMT][Chunk 8] ID-4 Y→G @local 500, saving\n",
            "02:54:55 INFO     [MGMT][Chunk 8] New best ID-4: 6 cars\n",
            "02:54:55 INFO     [MGMT][Chunk 8] Local 501 → Global 24600\n",
            "02:55:54 INFO     [MGMT][Chunk 8] Local 1001 → Global 25100\n",
            "02:56:06 INFO     [MGMT][Chunk 8] ID-1 Y→G @local 1100, saving\n",
            "02:56:06 INFO     [MGMT][Chunk 8] New best ID-1: 24 cars\n",
            "02:56:51 INFO     [MGMT][Chunk 8] Local 1501 → Global 25600\n",
            "02:57:48 INFO     [MGMT][Chunk 8] ID-3 Y→G @local 1970, saving\n",
            "02:57:48 INFO     [MGMT][Chunk 8] New best ID-3: 2 cars\n",
            "02:57:51 INFO     [MGMT][Chunk 8] Local 2001 → Global 26100\n",
            "02:58:49 INFO     [MGMT][Chunk 8] Local 2501 → Global 26600\n",
            "02:59:21 INFO     [Chunk 8] DONE in 323.0s\n",
            "02:59:21 INFO     [MGMT][Chunk 8] Generating recommendation JSON\n",
            "02:59:21 INFO     [MGMT][Chunk 8] Recommendations saved\n",
            "02:59:23 INFO     Done with management!\n",
            "02:59:23 INFO     All GPU tasks done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_eoL_vbRP_Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final code"
      ],
      "metadata": {
        "id": "vbt4Zgjsc4Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics --quiet\n",
        "print(\"Ultralytics installed successfully!\")\n",
        "\n",
        "\n",
        "# ✅ Install FilterPy (required by SORT)\n",
        "!pip install filterpy --quiet\n",
        "\n",
        "import os, shutil, sys\n",
        "\n",
        "# ✅ Clean up old files if they exist\n",
        "if os.path.exists('/kaggle/working/sort.py'):\n",
        "    os.remove('/kaggle/working/sort.py')\n",
        "if os.path.exists('/kaggle/working/sort'):\n",
        "    shutil.rmtree('/kaggle/working/sort')\n",
        "\n",
        "# ✅ Clone SORT repo\n",
        "!git clone https://github.com/abewley/sort.git /kaggle/working/sort\n",
        "\n",
        "# ✅ Add to Python path\n",
        "sys.path.append('/kaggle/working/sort')\n",
        "\n",
        "# ✅ Disable problematic visualization code\n",
        "sort_file = '/kaggle/working/sort/sort.py'\n",
        "with open(sort_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "with open(sort_file, 'w') as f:\n",
        "    for line in lines:\n",
        "        if \"matplotlib.use('TkAgg')\" in line:\n",
        "            f.write(\"# \" + line)\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "!pip install pymongo --quiet"
      ],
      "metadata": {
        "id": "T1JAIvRec83I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# 1) Imports & Config\n",
        "# ----------------------------------------\n",
        "\n",
        "import gc, cv2, os, csv, json, threading, multiprocessing, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from flask import Flask, jsonify\n",
        "from sort import Sort\n",
        "from pymongo import MongoClient\n",
        "import base64\n",
        "\n",
        "# Paths & constants\n",
        "VIDEO_IN       = \"/kaggle/input/videos/VideoInputStream.mp4\"\n",
        "CLIPS_DIR      = \"clips\"\n",
        "POLY_CSV       = \"/kaggle/input/videos/polygons.csv\"\n",
        "MODEL_PT       = \"/kaggle/input/videos/best (1).pt\"\n",
        "RECO_DIR       = Path(\"recommendations\")\n",
        "VIOL_DIR       = Path(\"violations\") # could save to database from violation's code\n",
        "OUT_DIR        = Path(\"outputs_video\") # Contains all the annotated chunks with frames for each annotated chunk\n",
        "LIGHT_DIR      = Path(\"original_lights\")\n",
        "COUNTS_DIR     = Path(\"counts\")\n",
        "BEST_FRAME_DIR = Path(\"best_frames\")\n",
        "Annotated_Videos = Path(\"Annotated_Videos\")\n",
        "\n",
        "# mongo setup\n",
        "MONGO_URI = os.getenv(\n",
        "    \"MONGO_URI\",\n",
        "    \"mongodb+srv://nafe:0597785625nafe@coffeeshop.s8duwhp.mongodb.net/?retryWrites=true&w=majority\"\n",
        ")\n",
        "\n",
        "DB_NAME  = os.getenv(\"DB_NAME\", \"trafficmanagement\")\n",
        "COL_NAME = os.getenv(\"COL_NAME\", \"records\")\n",
        "\n",
        "mongo_client = MongoClient(MONGO_URI)\n",
        "mongo_db     = mongo_client[DB_NAME]\n",
        "records      = mongo_db[COL_NAME]\n",
        "\n",
        "#Chunk boundaries\n",
        "CHUNK_RANGES = [\n",
        "    (0,2999),(3000,5999),(6000,8999),(9000,11999),\n",
        "    (12000,14999),(15000,18049),(18050,20999),\n",
        "    (21000,24099)\n",
        "]\n",
        "\n",
        "# # Chunk boundaries (global frame numbers)\n",
        "# CHUNK_RANGES = [\n",
        "#     (0,   2999),\n",
        "#     # (3000,5999),\n",
        "# ]\n",
        "\n",
        "# Ensure output dirs exist\n",
        "for d in (CLIPS_DIR, RECO_DIR, VIOL_DIR, OUT_DIR, LIGHT_DIR, COUNTS_DIR, BEST_FRAME_DIR,Annotated_Videos):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "for tid in (\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"):\n",
        "    (BEST_FRAME_DIR/tid).mkdir(exist_ok=True)\n",
        "\n",
        "def ccw(A, B, C):\n",
        "    # returns True if the points A, B, C are listed in counter‐clockwise order\n",
        "    return (C[1]-A[1])*(B[0]-A[0]) > (B[1]-A[1])*(C[0]-A[0])\n",
        "\n",
        "def segment_intersects(A, B, C, D):\n",
        "    # returns True if segment AB intersects segment CD\n",
        "    return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Splitting Logic\n",
        "# ----------------------------------------\n",
        "def split_into_chunks(video_path, output_dir, chunk_ranges):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    paths = []\n",
        "    for idx, (start, end) in enumerate(chunk_ranges):\n",
        "        print(f\"[SPLIT] Creating chunk {idx} frames {start}→{end}\")\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "        out_path = os.path.join(output_dir, f\"chunk_{idx}.mp4\")\n",
        "        writer = cv2.VideoWriter(out_path,\n",
        "                                 cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                                 fps, (w, h))\n",
        "        for _ in range(end - start + 1):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            writer.write(frame)\n",
        "        writer.release()\n",
        "        if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
        "            paths.append(out_path)\n",
        "            print(f\"[SPLIT] Saved {out_path}\")\n",
        "        else:\n",
        "            print(f\"[SPLIT] Removed empty {out_path}\")\n",
        "            os.remove(out_path)\n",
        "    cap.release()\n",
        "    return paths\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) data for management (includes: polygons for both intersections & traffic lights, colours, priority and weights)\n",
        "# ----------------------------------------\n",
        "poly_by_frame = {}\n",
        "with open(POLY_CSV, newline=\"\") as f:\n",
        "    for row in csv.DictReader(f):\n",
        "        fid, pid = int(row[\"frame\"]), row[\"id\"]\n",
        "        pts = np.array([[float(row[f\"x{i}\"]), float(row[f\"y{i}\"])]\n",
        "                        for i in range(1,5)], np.float32)\n",
        "        poly_by_frame.setdefault(fid, []).append((pid, pts))\n",
        "\n",
        "traffic_light_polygons = [\n",
        "    (\"ID-1\",  15,  83, 40,130),\n",
        "    (\"ID-2\", 105,  83, 40,130),\n",
        "    (\"ID-3\", 180,  83, 40,130),\n",
        "    (\"ID-4\", 270,  83, 40,130),\n",
        "]\n",
        "COLOURS   = [(0,255,0),(0,128,255),(255,0,0),(128,0,255)]\n",
        "TL_COLOUR = {\"red\":(0,0,255),\"yellow\":(0,255,255),\"green\":(0,255,0),\"unknown\":(128,128,128)}\n",
        "PRIORITY  = {\"red\":3,\"yellow\":2,\"green\":1}\n",
        "WEIGHTS   = {\"ID-2\":2,\"ID-4\":2,\"ID-1\":1,\"ID-3\":1}\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Traffic Management\n",
        "# ----------------------------------------\n",
        "def run_traffic_management(chunk_path: str, model: YOLO) -> None:\n",
        "    chunk_id = int(Path(chunk_path).stem.split(\"_\")[-1])\n",
        "    start, end = CHUNK_RANGES[chunk_id]\n",
        "\n",
        "    cap = cv2.VideoCapture(chunk_path)\n",
        "    w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    writer = cv2.VideoWriter(\n",
        "        str(Annotated_Videos / f\"annotated_chunk_{chunk_id}.mp4\"),\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h)\n",
        "    )\n",
        "\n",
        "    # instead of Sort() use:\n",
        "    tracker = Sort(max_age=360, min_hits=0, iou_threshold=0.005)\n",
        "    crossings = {tid: 0 for tid in (\"ID-1\", \"ID-2\", \"ID-3\", \"ID-4\")}\n",
        "    for tid in crossings:\n",
        "        all_frames_dir = BEST_FRAME_DIR / tid / \"all_frames\"\n",
        "        all_frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    seen_ids = {tid: set() for tid in crossings}\n",
        "    counting_active = {tid: False for tid in crossings}\n",
        "    prev_states = {tid: \"unknown\" for tid in crossings}\n",
        "    prev_tl_state = prev_states.copy()\n",
        "    best_car_counts = {tid: -1 for tid, *_ in traffic_light_polygons}\n",
        "    countdown_timer  = {tid: 0     for tid in crossings}\n",
        "\n",
        "    local_idx = 1\n",
        "    last_polys = []\n",
        "\n",
        "    last_positions = {}  # (pid, obj_id) → previous center\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        global_idx = start + local_idx - 1\n",
        "        if global_idx in poly_by_frame:\n",
        "            polys = [(pid, poly.astype(np.int32)) for pid, poly in poly_by_frame[global_idx]]\n",
        "            last_polys = polys\n",
        "        else:\n",
        "            polys = last_polys\n",
        "\n",
        "        # Run YOLO\n",
        "        res = model(frame, conf=0.2, verbose=False, show_labels=False)[0]\n",
        "        boxes = res.boxes\n",
        "        detections = []\n",
        "        for b in boxes:\n",
        "            cls = int(b.cls[0])\n",
        "            if cls != 0:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = b.xyxy[0]\n",
        "            conf = b.conf[0].item()\n",
        "            detections.append([x1.item(), y1.item(), x2.item(), y2.item(), conf])\n",
        "        tracked = tracker.update(np.array(detections))\n",
        "\n",
        "        # Save YOLO format per frame\n",
        "        yolo_lines = []\n",
        "        for b in boxes:\n",
        "            cls = int(b.cls[0])\n",
        "            x1, y1, x2, y2 = b.xyxy[0]\n",
        "            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "            bw, bh = (x2 - x1), (y2 - y1)\n",
        "            yolo_lines.append(f\"{cls} {cx/w:.6f} {cy/h:.6f} {bw/w:.6f} {bh/h:.6f}\")\n",
        "        (OUT_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.yolo.txt\").write_text(\"\\n\".join(yolo_lines))\n",
        "\n",
        "        # Count cars per polygon\n",
        "        counts = {pid: 0 for pid, _ in polys}\n",
        "        for ln in yolo_lines:\n",
        "            cls, cxn, cyn, *_ = ln.split()\n",
        "            if int(cls) != 0: continue\n",
        "            cx, cy = float(cxn) * w, float(cyn) * h\n",
        "            for pid, poly in polys:\n",
        "                if cv2.pointPolygonTest(poly, (cx, cy), False) >= 0:\n",
        "                    counts[pid] += 1\n",
        "                    break\n",
        "\n",
        "        # Detect traffic light states\n",
        "        tl_state = {tid: \"unknown\" for tid, *_ in traffic_light_polygons}\n",
        "        for ln in yolo_lines:\n",
        "            cls, cxn, cyn, *_ = ln.split()\n",
        "            cid = int(cls)\n",
        "            if cid not in (1, 2, 3): continue\n",
        "            colour = {1: \"green\", 2: \"red\", 3: \"yellow\"}[cid]\n",
        "            cx, cy = float(cxn) * w, float(cyn) * h\n",
        "            for tid, px, py, pw, ph in traffic_light_polygons:\n",
        "                if px <= cx <= px + pw and py <= cy <= py + ph:\n",
        "                    if PRIORITY[colour] > PRIORITY.get(tl_state[tid], 0):\n",
        "                        tl_state[tid] = colour\n",
        "                    break\n",
        "\n",
        "        # Manage Y→G transitions\n",
        "        for tid in tl_state:\n",
        "            if ((prev_tl_state[tid] == \"red\" and tl_state[tid] == \"yellow\"   ) or\n",
        "                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"yellow\") or\n",
        "                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"green\" ) or\n",
        "                (prev_tl_state[tid] == \"green\" and tl_state[tid] == \"green\"  )   ):\n",
        "                counting_active[tid] = True\n",
        "            elif (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"red\"):\n",
        "                countdown_timer[tid] = local_idx + 25\n",
        "            elif (tl_state[tid] == \"red\" and local_idx > countdown_timer[tid]):\n",
        "                counting_active[tid] = False\n",
        "            prev_states[tid] = tl_state[tid]\n",
        "\n",
        "\n",
        "        # Define crossing edges\n",
        "        crossing_edges = {}\n",
        "        for pid, poly in polys:\n",
        "            if len(poly) >= 2:\n",
        "                p1, p2 = poly[0], poly[1]\n",
        "                vec = p2 - p1\n",
        "                norm = np.linalg.norm(vec)\n",
        "                if norm < 1e-5:\n",
        "                    continue\n",
        "                if pid == \"ID-1\":\n",
        "                    shrink_ratio = 0.05\n",
        "                    p1 = p1 + vec * shrink_ratio\n",
        "                    p2 = p2 - vec * shrink_ratio\n",
        "                elif pid == \"ID-2\":\n",
        "                    shrink_ratio = 0.17\n",
        "                    p1 = p1 + vec * shrink_ratio\n",
        "                    p2 = p2 - vec * shrink_ratio\n",
        "                elif pid == \"ID-3\":\n",
        "                    shrink_ratio = 0.2\n",
        "                    p1 = p1 + vec * shrink_ratio\n",
        "                    p2 = p2 - vec * shrink_ratio\n",
        "                elif pid == \"ID-4\":\n",
        "                    shrink_ratio = 0.2\n",
        "                    p1 = p1 + vec * shrink_ratio\n",
        "                    p2 = p2 - vec * shrink_ratio\n",
        "\n",
        "\n",
        "                # Perpendicular vector for offset\n",
        "                perp = np.array([-vec[1], vec[0]]) / norm\n",
        "                if pid in (\"ID-2\", \"ID-3\", \"ID-4\"):\n",
        "                    perp = -perp\n",
        "                c1 = (p1 + perp * 5).astype(int)\n",
        "                c2 = (p2 + perp * 5).astype(int)\n",
        "                crossing_edges[pid] = (c1, c2)\n",
        "\n",
        "\n",
        "        # Crossing detection using SORT IDs\n",
        "        speed_threshold = 2.0  # px/frame\n",
        "\n",
        "        for x1, y1, x2, y2, obj_id in tracked:\n",
        "            obj_id = int(obj_id)\n",
        "            cx = (x1 + x2) / 2\n",
        "            cy = (y1 + y2) / 2\n",
        "            center = np.array([cx, cy])\n",
        "\n",
        "            for pid, (c1, c2) in crossing_edges.items():\n",
        "                if not counting_active[pid] or obj_id in seen_ids[pid]:\n",
        "                    continue\n",
        "\n",
        "                key = (pid, obj_id)\n",
        "                prev_center = last_positions.get(key)\n",
        "                if prev_center is not None:\n",
        "                    # 1) speed filter\n",
        "                    speed = np.linalg.norm(center - prev_center)\n",
        "                    # if speed < speed_threshold:\n",
        "                    #     # too slow / jittery — skip\n",
        "                    #     last_positions[key] = center\n",
        "                    #     continue\n",
        "\n",
        "                    # 2) true intersection test\n",
        "                    if segment_intersects(prev_center, center, c1, c2):\n",
        "                        crossings[pid] += 1\n",
        "                        seen_ids[pid].add(obj_id)\n",
        "\n",
        "                # update last position for next frame\n",
        "                last_positions[key] = center\n",
        "\n",
        "        # Overlay\n",
        "        # 1) Draw each tracked bounding box + its track ID\n",
        "        for x1, y1, x2, y2, obj_id in tracked:\n",
        "            x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 0), 2)             # cyan boxes\n",
        "            cv2.putText(frame, f\"ID{int(obj_id)}\", (x1, y1-5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 2)\n",
        "\n",
        "        # 2) Draw each ROI polygon in a distinct color\n",
        "        for i, (pid, poly) in enumerate(last_polys):\n",
        "            col = COLOURS[i % len(COLOURS)]\n",
        "            cv2.polylines(frame, [poly], True, col, 2)\n",
        "\n",
        "        # 3) Draw each crossing line (c1→c2)\n",
        "        for pid, (c1, c2) in crossing_edges.items():\n",
        "            cv2.line(frame, tuple(c1), tuple(c2), (255,255,255), 2)  # white line\n",
        "            # optional: label which line belongs to which PID\n",
        "            mid = ((c1+c2)//2).tolist()\n",
        "            cv2.putText(frame, pid, tuple(mid),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
        "\n",
        "        for i,(pid,poly) in enumerate(polys):\n",
        "            col = COLOURS[i%len(COLOURS)]\n",
        "            cv2.polylines(frame,[poly],True,col,2)\n",
        "            cv2.putText(frame,f\"{pid}:{counts[pid]}\",(w-300,40+i*40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,1.0,col,3)\n",
        "\n",
        "        writer.write(frame)\n",
        "\n",
        "        for tid in tl_state:\n",
        "            # frames_dir = BEST_FRAME_DIR / tid / \"all_frames\"\n",
        "            # frame_path = frames_dir / f\"chunk{chunk_id}_frame_{local_idx:06d}.jpg\"\n",
        "            # cv2.imwrite(str(frame_path), frame)\n",
        "\n",
        "            # existing best-frame logic\n",
        "            if ((prev_tl_state[tid] == \"red\"    and tl_state[tid] == \"yellow\") or\n",
        "                (prev_tl_state[tid] == \"yellow\" and tl_state[tid] == \"yellow\")):\n",
        "                # save counts & light JSON as before\n",
        "                (COUNTS_DIR / f\"chunk{chunk_id}_frame_{local_idx:06d}.txt\") \\\n",
        "                    .write_text(json.dumps(counts))\n",
        "                (LIGHT_DIR  / f\"chunk{chunk_id}_frame_{local_idx:06d}.json\") \\\n",
        "                    .write_text(json.dumps({\"cars\": counts, \"lights\": tl_state}))\n",
        "\n",
        "                # now save best‐frame if it beats the previous record\n",
        "                c = counts.get(tid, 0)\n",
        "                if c > best_car_counts[tid]:\n",
        "                    best_car_counts[tid] = c\n",
        "                    best_dir = BEST_FRAME_DIR / tid\n",
        "                    # this remains in the parent tid folder\n",
        "                    bf_path = best_dir / f\"chunk{chunk_id}_best_frame.jpg\"\n",
        "                    cv2.imwrite(str(bf_path), frame)\n",
        "                    (best_dir / f\"chunk{chunk_id}_best_frame.json\") \\\n",
        "                      .write_text(json.dumps({\"cars\": counts, \"lights\": tl_state}))\n",
        "                    (best_dir / f\"chunk{chunk_id}_best_frame.txt\") \\\n",
        "                      .write_text(json.dumps(counts))\n",
        "\n",
        "        prev_tl_state = tl_state.copy()\n",
        "        local_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    gc.collect()\n",
        "\n",
        "    # Save chunk summary\n",
        "    (RECO_DIR / f\"crossings_chunk_{chunk_id}.json\").write_text(json.dumps(crossings, indent=2))\n",
        "\n",
        "    # === RECOMMENDATIONS ===\n",
        "    best_data = {}\n",
        "    for tid, *_ in traffic_light_polygons:\n",
        "        p = BEST_FRAME_DIR / tid / f\"chunk{chunk_id}_best_frame.json\"\n",
        "        best_data[tid] = json.loads(p.read_text()) if p.exists() else {\"cars\": {tid: 0}, \"lights\": {tid: \"unknown\"}}\n",
        "\n",
        "    weighted = {tid: best_data[tid][\"cars\"].get(tid, 0) * WEIGHTS[tid] for tid in best_data}\n",
        "    candidates = list(weighted)\n",
        "    recs = []\n",
        "\n",
        "    for current in (\"ID-4\", \"ID-1\", \"ID-3\", \"ID-2\"):\n",
        "        rec = max(candidates, key=lambda t: weighted[t])\n",
        "        data = best_data[rec]\n",
        "        all_counts = {tid: cnt + 2 for tid, cnt in data[\"cars\"].items()}\n",
        "        all_states = data[\"lights\"].copy()\n",
        "        all_states[rec] = \"yellow\"\n",
        "        recs.append({\n",
        "            \"current\": current,\n",
        "            \"recommended\": rec,\n",
        "            \"duration_sec\": (all_counts[rec] * 2 + 2),\n",
        "            \"all_counts\": all_counts,\n",
        "            \"all_states\": all_states\n",
        "        })\n",
        "        candidates.remove(rec)\n",
        "\n",
        "    (RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\").write_text(json.dumps(recs, indent=2))\n",
        "    print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n",
        "# ----------------------------------------\n",
        "# 5) (Your violation detection follows…)\n",
        "# ----------------------------------------\n",
        "def run_violation_detection(chunk_path: str, _model=None) -> None:\n",
        "    # … unchanged …\n",
        "    pass\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) GPU Worker & Orchestrator\n",
        "# ----------------------------------------\n",
        "def gpu_worker(q):\n",
        "    model = YOLO(MODEL_PT)\n",
        "    while True:\n",
        "        task = q.get()\n",
        "        if task is None: break\n",
        "        kind, chunk = task\n",
        "        if kind==\"management\":\n",
        "            run_traffic_management(chunk, model)\n",
        "        else:\n",
        "            print(f\"[MGMT][Chunk {chunk_id}] Recommendations saved\")\n",
        "            # run_violation_detection(chunk, model)\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route(\"/reco/<chunk_id>\")\n",
        "def get_reco(chunk_id):\n",
        "    return jsonify(json.load(open(f\"{RECO_DIR}/chunk_{chunk_id}_recommendations.json\")))\n",
        "\n",
        "def run_simulation_loop(reco_dir, viol_dir):\n",
        "    while True: pass\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    # right before you call split_into_chunks(...)\n",
        "    CHUNKS_FILE = \"chunks_paths.txt\"\n",
        "\n",
        "    if os.path.exists(CHUNKS_FILE):\n",
        "        # Load from previous run\n",
        "        with open(CHUNKS_FILE, \"r\") as f:\n",
        "            chunks = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"[LOAD] Loaded {len(chunks)} chunk paths from {CHUNKS_FILE}\")\n",
        "    else:\n",
        "        # First time: actually split\n",
        "        chunks = split_into_chunks(VIDEO_IN, CLIPS_DIR, CHUNK_RANGES)\n",
        "        with open(CHUNKS_FILE, \"w\") as f:\n",
        "            for p in chunks:\n",
        "                f.write(p + \"\\n\")\n",
        "        print(f\"[SPLIT] Saved {len(chunks)} chunk paths to {CHUNKS_FILE}\")\n",
        "\n",
        "    task_q = multiprocessing.Queue()\n",
        "    p = multiprocessing.Process(target=gpu_worker, args=(task_q,))\n",
        "    p.start()\n",
        "\n",
        "    for c in chunks:\n",
        "        task_q.put((\"management\", c))\n",
        "        # task_q.put((\"violation\",  c))\n",
        "    task_q.put(None)\n",
        "\n",
        "    # threading.Thread(target=run_simulation_loop,\n",
        "    #                  args=(RECO_DIR,VIOL_DIR), daemon=True).start()\n",
        "    # threading.Thread(target=lambda: app.run(port=8888, host=\"0.0.0.0\"),\n",
        "    #                  daemon=True).start()\n",
        "\n",
        "    p.join()\n",
        "    print(\"All GPU tasks done.\")"
      ],
      "metadata": {
        "id": "7ZJzR-tCdfRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# READ FROM DISK & UPLOAD TO MONGODB (with real_world from crossings)\n",
        "# ----------------------------------------\n",
        "from pathlib import Path\n",
        "import json, base64, os\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# 1) Paths & constants\n",
        "CLIPS_DIR       = Path(\"clips\")\n",
        "RECO_DIR        = Path(\"recommendations\")\n",
        "BEST_FRAME_DIR  = Path(\"best_frames\")\n",
        "IDS             = [\"ID-1\",\"ID-2\",\"ID-3\",\"ID-4\"]\n",
        "\n",
        "# 2) MongoDB setup\n",
        "MONGO_URI = os.getenv(\n",
        "    \"MONGO_URI\",\n",
        "    \"mongodb+srv://abubakernafe1:0597785625nafe@trafficmanagement.r28vab3.mongodb.net/?retryWrites=true&w=majority\"\n",
        ")\n",
        "DB_NAME  = os.getenv(\"DB_NAME\", \"trafficmanagement\")\n",
        "COL_NAME = os.getenv(\"COL_NAME\", \"records\")\n",
        "client   = MongoClient(MONGO_URI)\n",
        "db       = client[DB_NAME]\n",
        "records  = db[COL_NAME]\n",
        "\n",
        "# 3) Find all chunks by scanning clips/\n",
        "chunk_files = sorted(CLIPS_DIR.glob(\"chunk_*.mp4\"))\n",
        "for clip in chunk_files:\n",
        "    chunk_id = int(clip.stem.split(\"_\")[1])\n",
        "\n",
        "    # 4) Load recommendations JSON\n",
        "    rec_path = RECO_DIR / f\"chunk_{chunk_id}_recommendations.json\"\n",
        "    if not rec_path.exists():\n",
        "        print(f\"[WARN] missing recommendations for chunk {chunk_id}, skipping\")\n",
        "        continue\n",
        "    recs = json.loads(rec_path.read_text())\n",
        "\n",
        "    # 5) Build best_frames array\n",
        "    best_frames = []\n",
        "    for tid in IDS:\n",
        "        img_file = BEST_FRAME_DIR / tid / f\"chunk{chunk_id}_best_frame.jpg\"\n",
        "        if img_file.exists():\n",
        "            b64 = base64.b64encode(img_file.read_bytes()).decode(\"utf-8\")\n",
        "        else:\n",
        "            b64 = None\n",
        "        best_frames.append({\"id\": tid, \"image\": b64})\n",
        "\n",
        "    # 6) Read real-world crossings from crossings_chunk_{n}.json\n",
        "    cross_path = RECO_DIR / f\"crossings_chunk_{chunk_id}.json\"\n",
        "    if cross_path.exists():\n",
        "        crossings = json.loads(cross_path.read_text())\n",
        "    else:\n",
        "        crossings = {tid: 0 for tid in IDS}\n",
        "\n",
        "    real_world = [\n",
        "        {\"id\": tid, \"cars_passed_in_real\": crossings.get(tid, 0)}\n",
        "        for tid in IDS\n",
        "    ]\n",
        "\n",
        "    # 7) Build upsert document\n",
        "    doc = {\n",
        "        \"chunk\":           chunk_id,\n",
        "        \"video_path\":      str(CLIPS_DIR / f\"chunk_{chunk_id}.mp4\"),\n",
        "        \"best_frames\":     best_frames,\n",
        "        \"recommendations\": recs,\n",
        "        \"real_world\":      real_world\n",
        "    }\n",
        "\n",
        "    # 8) Upsert into Mongo\n",
        "    records.update_one(\n",
        "        {\"chunk\": chunk_id},\n",
        "        {\"$set\": doc},\n",
        "        upsert=True\n",
        "    )\n",
        "    print(f\"[DB] upserted chunk {chunk_id}\")"
      ],
      "metadata": {
        "id": "u6AY9DLpdh41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
